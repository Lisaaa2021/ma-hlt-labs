{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use BERT and RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way with an automatic process to generate inputs and labels from those texts. BERT was pretrained with two objectives:\n",
    "\n",
    "* Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the sentence.\n",
    "* Next sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining. Sometimes they correspond to sentences that were next to each other in the original text, sometimes not. The model then has to predict if the two sentences were following each other or not.\n",
    "\n",
    "The model learns an inner representation of the English language (a language model) that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard classifier using the features produced by the BERT model as inputs. Devlin et al 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a3da0309c2546438318d8eeb5856ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e15c8f925f4c80b492d36ab58b3bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "250dd371ff7c4d58ad707481655aa2e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline('fill-mask', model='bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr Smith was charged with murder.\n",
      "Mr Brown was charged with murder.\n",
      "Mr Jones was charged with murder.\n",
      "Mr Johnson was charged with murder.\n",
      "Mr Williams was charged with murder.\n"
     ]
    }
   ],
   "source": [
    "for res in pipe('Mr [MASK] was charged with murder.'):\n",
    "    print(res['sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr Williams was charged with murder.\n",
      "Mr Williams was charged with assault.\n",
      "Mr Williams was charged with theft.\n",
      "Mr Williams was charged with fraud.\n",
      "Mr Williams was charged with corruption.\n"
     ]
    }
   ],
   "source": [
    "for res in pipe('Mr Williams was charged with [MASK].'):\n",
    "    print(res['sequence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RoBERTa is a sequel to BERT in which the next sentence prediction was dropped, more data, more layers and wider text context was used. Liu et al 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. Conneau et al 2019: https://arxiv.org/pdf/1901.07291.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline('fill-mask', model='xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I wish I was a girl.\n",
      "I wish I was a teenager.\n",
      "I wish I was a poet.\n",
      "I wish I was a child.\n",
      "I wish I was a virgin.\n"
     ]
    }
   ],
   "source": [
    "for res in pipe('I wish I was a <mask>.'):\n",
    "    print(res['sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ik wou dat ik een vrouw was.\n",
      "Ik wou dat ik een meisje was.\n",
      "Ik wou dat ik een engel was.\n",
      "Ik wou dat ik een homo was.\n",
      "Ik wou dat ik een mens was.\n"
     ]
    }
   ],
   "source": [
    "for res in pipe('Ik wou dat ik een <mask> was.'):\n",
    "    print(res['sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ich mochte gerne ein MÃ¤dchen sein.\n",
      "Ich mochte gerne ein Moderator sein.\n",
      "Ich mochte gerne ein Mensch sein.\n",
      "Ich mochte gerne ein Paar sein.\n",
      "Ich mochte gerne ein Engel sein.\n"
     ]
    }
   ],
   "source": [
    "for res in pipe('Ich mochte gerne ein <mask> sein.'):\n",
    "    print(res['sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The man was charged with murder.\n",
      "The suspect was charged with murder.\n",
      "The woman was charged with murder.\n",
      "The victim was charged with murder.\n",
      "The officer was charged with murder.\n"
     ]
    }
   ],
   "source": [
    "for res in pipe('The <mask> was charged with murder.'):\n",
    "    print(res['sequence'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
