{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 How to use GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT (Generative Pretrained Transformer) is a model trained to generate text given a preceding input (Brown et al 2020) It can do this repetitively up to a certain length, likewise generating short stories.\n",
    "\n",
    "Another generative model is T5 (Text to Text Transfer Transformer), which models many tasks as text generation tasks (Raffel et al. 2019).\n",
    "\n",
    "In this notebook, we look into an older model GPT2, which is smaller and publicly available.\n",
    "\n",
    "### References\n",
    "\n",
    "Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n",
    "\n",
    "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text trans- former. arXiv preprint arXiv:1910.10683."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the **simpletransformer** package to download the model. This may take a while. While the model loads, you may read the documentation on:\n",
    "\n",
    "https://simpletransformers.ai\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from simpletransformers.language_generation import (\n",
    "    LanguageGenerationModel,\n",
    ")\n",
    "\n",
    "model = LanguageGenerationModel(\n",
    "    \"gpt2\", \"gpt2\", use_cuda=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you succesfully downloaded it, it is saved on disk in cache for futher use. The next time you load the model it will be faster from disk.\n",
    "\n",
    "Because we loaded the model through the **simpletransformer** class LanguageGenerationModel, we can use the function **generate** directly. Without a prompt, it will *prompt* you for input, with a prompt, will respond by completing the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Model prompt >>>  Hi how are you?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Hi how are you? Well, you're good… you're kind of like that old school big brother. And there's\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2 for other languages than English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a GPT model from scratch is costly. You not only need a lot of data but also computer power to create such a model. An interesting alternative is to only train the vocabulary part of a model for a language and to keep the hidden layers of the English model for the contextual attention relations and capability to predict the next token embeddings.\n",
    "\n",
    "This is what was done by de Vries and Nissime (2021) from Groningen University for Dutch and Italian. You can read the paper for more details.\n",
    "\n",
    "References:\n",
    "\n",
    "de Vries, Wietse, and Malvina Nissim. \"As good as new. How to successfully recycle English GPT-2 to make models for other languages.\" arXiv preprint arXiv:2012.05628 (2020). https://aclanthology.org/2021.findings-acl.74.pdf\n",
    "\n",
    "See also: https://github.com/wietsedv/gpt2-recycle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can download the models from Huggingface as we did for the English GPT2 and generate a Dutch and Italian short story from a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"Was ik maar een kind geweest?'\\nIk keek haar in de ogen. 'Wat is er met je gebeurd? Ik heb mijn ouders verloren.'\\nZe schudde heftig haar hoofd. 'Nee, we hadden het niet kunnen hebben gedaan,' zei ze terwijl ze naar achteren liep om te voorkomen dat zijn gezicht zou gaan bevriezen. 'Je hebt geen idee waar jij mee bezig was en wat hij ervan had weerhouden me zo snel mogelijk hiernaartoe te komen...'\\n'We wilden allebei weten wie die vent achter ons aan\"}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "dutchGpt2pipe = pipeline(\"text-generation\", model=\"GroNLP/gpt2-small-dutch\")\n",
    "print(dutchGpt2pipe('Was ik maar een klein'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Was ik maar een klein kind.\\'\\nHij knikte. Hij had haar niet verteld wat ze moest doen om hem te beschermen, en er was geen bewijs dat hij ooit in de buurt van zijn huis zou blijven rondhangen. \\'En dan is het nog steeds zo moeilijk als je me wilt vragen waarom jij dit allemaal hebt gedaan?\\'\\nHaar ogen vulden zich met tranen omdat ze niets kon zeggen over die vreselijke waarheid. De woorden waren al bijna twee weken uit hun hoofd geweest: \"Je kunt mijn leven ruïneren'}]\n"
     ]
    }
   ],
   "source": [
    " print(dutchGpt2pipe('Was ik maar een klein'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "italianGpt2pipe = pipeline(\"text-generation\", model=\"GroNLP/gpt2-small-italian\")\n",
    "\n",
    "print(italianGpt2pipe('Uno bambino picolo'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
