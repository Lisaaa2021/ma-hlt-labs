{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab4.4 How to use fine-tuned crosslinglual transformer models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer models such as BERT and RoBERTa can easily be fine-tuned for downstream tasks. The Huggingface model hub lists many of these models already trained for specific tasks. In this notebook, we show two examples of fine-tuned models for xlm-roberta. Because the language model is cross-lingual, also the fine-tuned model works for all the 100 languages that xlm-roberta models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We search on the Model Hub of Huggingface for a fine-tuned xlm-roberta model for sentiment classification, e.g.:\n",
    "\n",
    "https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "model_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "sentiment_task = pipeline(\"sentiment-analysis\", model=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'Negative', 'score': 0.9273003935813904}]\n",
      "[{'label': 'Negative', 'score': 0.8501137495040894}]\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_task(\"What an awful movie!\"))\n",
    "print(sentiment_task(\"Wat een waardeloze film!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"Davlan/xlm-roberta-base-ner-hrl\")\n",
    "#model = AutoModelForTokenClassification.from_pretrained(\"Davlan/xlm-roberta-base-ner-hrl\")\n",
    "nerc_task = pipeline(\"ner\", model=\"Davlan/xlm-roberta-base-ner-hrl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'B-PER', 'score': 0.9998415, 'index': 1, 'word': '▁Na', 'start': 0, 'end': 2}\n",
      "{'entity': 'I-PER', 'score': 0.88056284, 'index': 2, 'word': 'der', 'start': 2, 'end': 5}\n",
      "{'entity': 'I-PER', 'score': 0.99981594, 'index': 3, 'word': '▁Jo', 'start': 5, 'end': 8}\n",
      "{'entity': 'I-PER', 'score': 0.99980223, 'index': 4, 'word': 'kha', 'start': 8, 'end': 11}\n",
      "{'entity': 'I-PER', 'score': 0.999753, 'index': 5, 'word': 'dar', 'start': 11, 'end': 14}\n",
      "{'entity': 'B-LOC', 'score': 0.99962485, 'index': 8, 'word': '▁Syria', 'start': 24, 'end': 30}\n"
     ]
    }
   ],
   "source": [
    "example = \"Nader Jokhadar had given Syria the lead with a well-struck header in the seventh minute.\"\n",
    "nerc_results = nerc_task(example)\n",
    "for result in nerc_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'B-PER', 'score': 0.9998753, 'index': 1, 'word': '▁Mark', 'start': 0, 'end': 4}\n",
      "{'entity': 'I-PER', 'score': 0.99985516, 'index': 2, 'word': '▁Rut', 'start': 4, 'end': 8}\n",
      "{'entity': 'I-PER', 'score': 0.9998762, 'index': 3, 'word': 'te', 'start': 8, 'end': 10}\n",
      "{'entity': 'B-ORG', 'score': 0.999185, 'index': 9, 'word': '▁VVD', 'start': 29, 'end': 33}\n",
      "{'entity': 'B-ORG', 'score': 0.99986625, 'index': 13, 'word': '▁Google', 'start': 54, 'end': 61}\n",
      "{'entity': 'B-ORG', 'score': 0.99985904, 'index': 15, 'word': '▁Facebook', 'start': 62, 'end': 71}\n",
      "{'entity': 'B-ORG', 'score': 0.9998469, 'index': 17, 'word': '▁Apple', 'start': 74, 'end': 80}\n"
     ]
    }
   ],
   "source": [
    "example = \"Mark Rutte kondigt aan dat de VVD tech bedrijven zoals Google, Facebook en Apple zwaarder gaat belasten.\"\n",
    "nerc_results = nerc_task(example)\n",
    "for result in nerc_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
