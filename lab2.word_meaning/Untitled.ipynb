{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building embeddings from the emotion data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/.local/lib/python3.7/site-packages/ipykernel_launcher.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \n",
      "/Users/piek/.local/lib/python3.7/site-packages/ipykernel_launcher.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "filepath = '../lab3.machine_learning/data/MELD/train_sent_emo.csv'\n",
    "dftrain = pd.read_csv(filepath)\n",
    "### The data has some problematic strings with encoding problems. The next code removes some of these from the utterances\n",
    "# Try to fix encoding\n",
    "dftrain['Utterance'] = dftrain['Utterance'].str.replace(\"\\x92|\\x97|\\x91|\\x93|\\x94|\\x85\", \"'\")\n",
    "\n",
    "filepath = '../lab3.machine_learning/data/MELD/test_sent_emo.csv'\n",
    "dftest = pd.read_csv(filepath)\n",
    "dftest['Utterance'] = dftest['Utterance'].str.replace(\"\\x92|\\x97|\\x91|\\x93|\\x94|\\x85\", \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import string\n",
    "\n",
    "#Function to remove punctuation from word tokens, \n",
    "#Takes a list of tokens as input\n",
    "\n",
    "#Note that these functions only work if you also imported NLTK and string before calling the function\n",
    "def remove_punct(tokens):\n",
    "    # punct is a string with all punctuation tokens: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
    "    punct = string.punctuation\n",
    "    # empty list in which we put the clean tokens\n",
    "    tokens_clean = []\n",
    "\n",
    "    # Iterate over all characters in tokens \n",
    "    # and only keeps them if not in punct\n",
    "    for t in tokens:\n",
    "        if t not in punct:\n",
    "            tokens_clean.append(t)\n",
    "            \n",
    "    return tokens_clean\n",
    "# If you want to process other text than the Leipzig corpus that is not split into sentences,\n",
    "# you can call the next function. The difference is:\n",
    "# - we read the complete file as a text string\n",
    "# - we apply the NLTK sent_tokenize function to the get a list of sentences\n",
    "# - we do not need to remove the identifier\n",
    "def preprocess_rawtext(texts):\n",
    "    clean_sentences = []\n",
    "    for text in texts: \n",
    "        sentences = sent_tokenize(text.strip())\n",
    "\n",
    "        for sentence in sentences:\n",
    "            tokens = word_tokenize(sentence.lower())\n",
    "            tokens_clean = remove_punct(tokens)\n",
    "            clean_sentences.append(tokens_clean)\n",
    "    return clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### the first loop gets the utterances\n",
    "training_instances = dftrain['Utterance']\n",
    "### the first loop gets the utterances\n",
    "test_instances = dftest['Utterance']\n",
    "\n",
    "clean_sentences = preprocess_rawtext(training_instances)\n",
    "clean_sentences.extend(preprocess_rawtext(test_instances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences= 17757\n",
      "[['ross', 'can', 'i', 'talk', 'to', 'you', 'for', 'a', 'minute'], ['yes', 'please'], ['so', 'what', \"'s\", 'going', 'on'], ['uh', 'well', '...', 'joey', 'and', 'i', 'broke', 'up'], ['oh', 'my', 'god', 'wh-what', 'happened'], ['joey', 'is', 'a', 'great', 'guy', 'but', 'we', \"'re\", 'just', '...', 'so', 'different'], ['i', 'mean', 'during', 'your', 'speech', 'he', 'kept', 'laughing', 'at', 'homo', 'erectus']]\n"
     ]
    }
   ],
   "source": [
    "print('Number of sentences=',len(clean_sentences))\n",
    "#we print a few sentences to see how it looks like\n",
    "print(clean_sentences[201:208])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../lab3.machine_learning/data/wassa/training/all.train.tsv'\n",
    "dftweets_train = pd.read_csv(filepath, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_instances = dftweets_train['Tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences= 23770\n"
     ]
    }
   ],
   "source": [
    "clean_sentences.extend(preprocess_rawtext(tweet_instances))\n",
    "print('Number of sentences=',len(clean_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../lab3.machine_learning/data/smile/datasets-773597-1332516-smile-annotations-final.csv'\n",
    "smile = pd.read_csv(filepath, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences= 29783\n"
     ]
    }
   ],
   "source": [
    "smile_instances = dftweets_train['Tweet']\n",
    "clean_sentences.extend(preprocess_rawtext(tweet_instances))\n",
    "print('Number of sentences=',len(clean_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to do the next commands only once. When you have succesfully created and saved the embeddings you can load them afterwards\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v = Word2Vec(clean_sentences, size=30, window =4, min_count =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size = 30\n"
     ]
    }
   ],
   "source": [
    "print('Vector size =', w2v.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "[ 0.18709244  0.6194727   0.74906373 -0.91969603 -0.7626541  -0.6097418\n",
      " -0.5105595  -0.19118078 -0.12841226  0.6019333   1.7183944  -0.34294078\n",
      " -0.144883   -1.1201503  -0.41258347 -0.52747124  0.1678897   0.72965366\n",
      " -0.5247881  -1.6217178   0.09158331 -1.3900898   1.0378044  -1.5165662\n",
      " -0.526757   -0.3254857  -0.6291967   0.3353514  -1.3505806  -0.58544546]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/.local/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "king_vector = w2v[\"guy\"]\n",
    "print(len(king_vector))\n",
    "print(king_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/.local/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('lol', 0.9983160495758057),\n",
       " ('negative', 0.9983031749725342),\n",
       " ('thanks', 0.9979099035263062),\n",
       " ('v', 0.9977244734764099),\n",
       " ('indian', 0.9976120591163635),\n",
       " ('mr.', 0.9974886775016785),\n",
       " ('hello', 0.9970127940177917),\n",
       " ('haha', 0.996992290019989),\n",
       " ('action', 0.9967154860496521),\n",
       " (\"y'all\", 0.9965254068374634)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.similar_by_word(\"boy\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
