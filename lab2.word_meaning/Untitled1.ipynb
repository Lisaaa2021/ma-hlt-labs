{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf93d99a-21f6-4da8-b9a8-946ba32df052",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1115706e-d7b4-44a2-b7e9-147b8d24fdd1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd39d736-4445-4e55-84f0-5d0e09ba631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29d8abd-2deb-4bf7-a0ed-a2e729e911f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61559d25-4c05-4e88-9e35-ad6b20130e6d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.2.0-cp39-cp39-macosx_10_9_x86_64.whl (24.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.22.0)\n",
      "Installing collected packages: gensim\n",
      "Successfully installed gensim-4.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f1e69e-75ac-443e-94ea-bea2b34948b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc262857-2004-4186-bf56-19eb895bedf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958d7f42-b972-4373-8d22-1e3a25d6e5d9",
   "metadata": {},
   "source": [
    "Initialize and train a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db494052-faa3-434b-95dc-fbaf8e06a11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# get some test sentences\n",
    "from gensim.test.utils import common_texts  # some example sentences\n",
    "print(len(common_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5d942e-d8bb-4a99-9aa7-621e0cdc0637",
   "metadata": {},
   "source": [
    "Let's checkout the format of these sentences. There are only nine of those so we can print them all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83754d54-935d-4856-9ca9-313c63ecc2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "print(common_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a68a46-33f8-4a9f-9e40-7125de60fea4",
   "metadata": {},
   "source": [
    "You can see that the text was preprocessed and only a list of content words was kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9a49759-dae3-4964-84fd-4dc754fdbbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 290)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FastText(vector_size=4, window=3, min_count=1)  # instantiate\n",
    "model.build_vocab(corpus_iterable=common_texts)\n",
    "model.train(corpus_iterable=common_texts, total_examples=len(common_texts), epochs=10)  # train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbaa6efe-b86c-4684-b0e2-416f514a4c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = FastText(vector_size=4, window=3, min_count=1, sentences=common_texts, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1e0a475-85d9-4150-98d0-b79ef99f460d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(model.wv['computer'], model2.wv['computer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d19f560-8c08-45eb-a0b1-72c9d24250fd",
   "metadata": {},
   "source": [
    "https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.LineSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af56eef8-6039-4cfd-be8d-564b4b9f3aae",
   "metadata": {
    "tags": []
   },
   "source": [
    "Loading a fastText word embedding model for a language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c97d90e-04ea-411a-b75e-8c7de8cfa8d9",
   "metadata": {},
   "source": [
    "On this GitHub repository, you find word embeddings models built for 150 languages by Facebook:\n",
    "\n",
    "https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md\n",
    "\n",
    "The text versions have the extension \".vec\" in their file name and can be read. The binary version have the extension \".bin\" and should be loaded with the FastText module.\n",
    "\n",
    "Download a model of your choice to disk, as I did for Limburgish and Afrikaans. You can now load it in gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cf4efd8-d0bc-4b5a-be69-565037448696",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '\\xba'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6w/bw7dqbl9727c48pcjjh32r140000gn/T/ipykernel_57387/541922929.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath_to_my_fasttext_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/piek/Desktop/t-ONDERWIJS/data/word-embeddings/fasttext_we_157/cc.fy.300.bin\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFastText\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_my_fasttext_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/models/fasttext.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \"\"\"\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFastText\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrethrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, rethrow, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1937\u001b[0m         \"\"\"\n\u001b[1;32m   1938\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1939\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1940\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1941\u001b[0m                 \u001b[0mrethrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_lifecycle_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \"\"\"\n\u001b[1;32m   1460\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1461\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# needed because loading from S3 doesn't support readline()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '\\xba'."
     ]
    }
   ],
   "source": [
    "path_to_my_fasttext_model = \"/Users/piek/Desktop/t-ONDERWIJS/data/word-embeddings/fasttext_we_157/cc.fy.300.bin\"\n",
    "model = FastText.load(path_to_my_fasttext_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a19008c0-c1a2-4c11-8524-aae6abe71bc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fasttext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6w/bw7dqbl9727c48pcjjh32r140000gn/T/ipykernel_71316/1264288433.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fasttext'"
     ]
    }
   ],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444050b1-bed2-4390-8076-10bfbb1e3c22",
   "metadata": {},
   "source": [
    "Once loaded, such models behave identically to those created from scratch. For example, you can continue training the loaded model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0cd9f5-85eb-4bc7-96f9-9a8f48d7ac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'computation' in model.wv.key_to_index  # New word, currently out of vocab\n",
    "\n",
    "old_vector = np.copy(model.wv['computation'])  # Grab the existing vector\n",
    "new_sentences = [['computer', 'aided', 'design'],['computer', 'science'],['computational', 'complexity'],['military', 'supercomputer'], ['central', 'processing', 'unit'],['onboard', 'car', 'computer'],]\n",
    "\n",
    "model.build_vocab(new_sentences, update=True)  # Update the vocabulary\n",
    "model.train(new_sentences, total_examples=len(new_sentences), epochs=model.epochs)\n",
    "\n",
    "new_vector = model.wv['computation']\n",
    "np.allclose(old_vector, new_vector, atol=1e-4)  # Vector has changed, model has learnt something\n",
    "\n",
    "'computation' in model.wv.key_to_index  # Word is still out of vocab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hlt_env",
   "language": "python",
   "name": "hlt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
