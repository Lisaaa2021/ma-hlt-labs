{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB2.2: Word embeddings from Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will introduce you to word embeddings. Word embeddings are vector representations for words learned by a neural network to predict words that occur in their context from a very large corpus. The weights applied to the hidden layer to make the correct predictions are taken as the vector representation for the meaning of the word. Usually, vector sizes are limited to 300 to 500 dimensions (context words). The advantage is that comparing vectors across words always match for certain dimensions: i.e. the vectors are dense vectors but match most strongly when words occur in similar contexts.\n",
    "\n",
    "Although there are many packages and data sets with embeddings, we focus on publicly available and trainable embeddings, especially for multiple languages. We therefore use the wikipedia2vec package that has pretrained models in various languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acnowledgement: https://wikipedia2vec.github.io/wikipedia2vec/\n",
    "\n",
    "To use the embeddings created from wikipedia (in a specific language) you need to do 3 things (also described in the above website):\n",
    "\n",
    "* Use `pip install Wikipedia2Vec` from the command line/terminal to install the package on your local computer\n",
    "* download an embedding model trained from wikipedia and unpack the compressed file with a decompression application\n",
    "* import the package in your notebook\n",
    "* load the local copy of the embedding model\n",
    "\n",
    "We guide you through these steps in this notebook and explain the basic functions. As there are different models for different languages, you can do this for any of the available languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `pip install Wikipedia2Vec` on the command line to install the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Wikipedia2Vec in /Users/piek/opt/anaconda3/lib/python3.7/site-packages (1.0.4)\n",
      "Requirement already satisfied: click in /Users/piek/opt/anaconda3/lib/python3.7/site-packages (from Wikipedia2Vec) (7.0)\n",
      "Requirement already satisfied: marisa-trie in /Users/piek/opt/anaconda3/lib/python3.7/site-packages (from Wikipedia2Vec) (0.7.5)\n",
      "Requirement already satisfied: mwparserfromhell in /Users/piek/opt/anaconda3/lib/python3.7/site-packages (from Wikipedia2Vec) (0.5.4)\n",
      "Requirement already satisfied: numpy in /Users/piek/opt/anaconda3/lib/python3.7/site-packages (from Wikipedia2Vec) (1.20.0)\n",
      "Requirement already satisfied: tqdm in /Users/piek/opt/anaconda3/lib/python3.7/site-packages (from Wikipedia2Vec) (4.62.2)\n",
      "Requirement already satisfied: six in /Users/piek/opt/anaconda3/lib/python3.7/site-packages (from Wikipedia2Vec) (1.15.0)\n",
      "Requirement already satisfied: jieba in /Users/piek/opt/anaconda3/lib/python3.7/site-packages (from Wikipedia2Vec) (0.42.1)\n",
      "Requirement already satisfied: lmdb in /Users/piek/opt/anaconda3/lib/python3.7/site-packages (from Wikipedia2Vec) (1.0.0)\n",
      "Requirement already satisfied: scipy in /Users/piek/opt/anaconda3/lib/python3.7/site-packages (from Wikipedia2Vec) (1.5.4)\n",
      "Requirement already satisfied: joblib in /Users/piek/opt/anaconda3/lib/python3.7/site-packages (from Wikipedia2Vec) (1.0.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/Users/piek/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install Wikipedia2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download pre-trained models in various languages from: https://wikipedia2vec.github.io/wikipedia2vec/pretrained/\n",
    "\n",
    "Note that there are different variants trained for 100 and 300 dimensions. If your computer has limited capacity, it is better to start with the 100 dimensions. For this notebook, we will download enwiki_20180420_100d.pkl.bz2, which is a compressed version of the 100 dimensions embedding model built from the English Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE!\n",
    "\n",
    "If you fail to install the Wikipedia2Vec package, do not waste too much time fixing this. You can also switch to the notebook *Lab2.2b.Wikipedia2Vec_Gensim.ipynb* which explains how you can load the Wikipedia2Vec  text models in another package *Gensim*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you succesfully install Wikipedia2Vec you can proceed here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When installed succesfully you can use the next import in your notebook. There is no need to install it again. \n",
    "from wikipedia2vec import Wikipedia2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to download a model in a format that Wikipedia2Vec can load. The binary versions (pkl) are compressed in a bz2 format. You need to decompress the bz2 file to a file with the extension \".pkl\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the path to your local copy of an embedding model.\n",
    "# Here we specify an example of such a path. Adapt the path to where you have stored the donwload\n",
    "# Make sure it is decompressed. The *.bz2 file will not load. \n",
    "\n",
    "MODEL_FILE='/Users/piek/Desktop/t-ONDERWIJS/data/word-embeddings/wiki2vec/enwiki_20180420_100d.pkl'\n",
    "wiki2vec = Wikipedia2Vec.load(MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By loading the model, we created an object with the name \"wiki2vec\" through which we can call functions and attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '_build_entity_neg_table',\n",
       " '_build_uniform_neg_table',\n",
       " '_build_unigram_neg_table',\n",
       " '_build_word_neg_table',\n",
       " 'dictionary',\n",
       " 'get_entity',\n",
       " 'get_entity_vector',\n",
       " 'get_vector',\n",
       " 'get_word',\n",
       " 'get_word_vector',\n",
       " 'load',\n",
       " 'load_text',\n",
       " 'most_similar',\n",
       " 'most_similar_by_vector',\n",
       " 'save',\n",
       " 'save_text',\n",
       " 'syn0',\n",
       " 'syn1',\n",
       " 'train',\n",
       " 'train_params']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(wiki2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Accessing word representations of different models\n",
    "\n",
    "Models may be stored in different (sometimes in confusing) formats, but they all boil down to these components:\n",
    "\n",
    "* a matrix of word vectors \n",
    "* a vocabulary\n",
    "* a mapping between vectors in the matrix to the words in the vocabulary (often via indices)\n",
    "\n",
    "Think about what a matrix is (no not the movie). You know that a vector is a list of digits, such that each digit is a value for a dimension in an n-dimensional space. Well, if you have a list of these vectors you have a matrix of n-columns and m-rows. Each row corresponds to the vector of a word in the vocabulary.\n",
    "\n",
    "The matrix of 3 rows and 3 columns\n",
    "```\n",
    "[[.34, .56, ,12],\n",
    " [.12, .39, ,05],\n",
    " [.78, .37, ,01]]\n",
    "```\n",
    "\n",
    "The vocabulary with the word as a key and the matrix list index that points to the row with the embedding for the word:\n",
    "\n",
    "```{\"dog\": 0, \"cat\" : 1, \"car\" : 2}```\n",
    "\n",
    "For this data, a simple lookup function for *dog* will give the embedding *[.34, .56, ,12]*.\n",
    "\n",
    "Now let's see how this is implemented in Wikipedia2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 The vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is represented internally as a...\n",
      "<class 'wikipedia2vec.wikipedia2vec.Wikipedia2Vec'>\n"
     ]
    }
   ],
   "source": [
    "# Explore the wiki2vec model as a python object:\n",
    "print('The model is represented internally as a...')\n",
    "print(type(wiki2vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has a dictionary that contains words. Let's check how big the vocabulary is of the model derived from English Wikipedia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model vocabulary is represented internally as a...\n",
      "<class 'wikipedia2vec.dictionary.Dictionary'>\n",
      "1937422\n"
     ]
    }
   ],
   "source": [
    "vocabulary = wiki2vec.dictionary\n",
    "print('The model vocabulary is represented internally as a...')\n",
    "print(type(vocabulary))\n",
    "print(len(list(vocabulary.words())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost two millions words are present in this model. That is a lot more than in the English WordNet. Let's check some of these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some words from the model vocabulary:\n",
      "[<Word s>, <Word sa>, <Word san>, <Word sand>, <Word sanda>, <Word sandal>, <Word sandali>, <Word sandalia>, <Word sandaliatus>, <Word sandalinas>, <Word sandaling>, <Word sandalio>, <Word sandaliyah>, <Word sandalo>, <Word sandalodes>, <Word sandalon>, <Word sandalops>, <Word sandalore>, <Word sandalov>, <Word sandalow>]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#####\n",
    "print('Some words from the model vocabulary:')\n",
    "print(list(vocabulary.words())[:20]) #Note that :20 gives the first 20 items in the list, print(list(vocabulary.words())[-1]) gives the last word\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 The embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word in the vocabulary, we can now get the vector. We assume that 'man' is in the vocabulary. We can use the *get_word_vector* function to lookup the vector from the matrix for the word 'man':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information stored in the vocabulary for the word \"man\":\n",
      "<class 'numpy.memmap'>\n",
      "Distributional meaning of \"man\" in Wikipedia: [-2.37749144e-01  2.68582195e-01 -9.62369144e-02  2.70704746e-01\n",
      " -2.24097610e-01 -2.48913109e-01  1.06461413e-01  4.12168130e-02\n",
      " -5.34945190e-01 -1.44513458e-01 -8.70477855e-02 -1.87745050e-01\n",
      "  1.98523641e-01 -1.64299533e-01  1.02062628e-01 -1.78317577e-01\n",
      " -5.51789738e-02  2.19180398e-02 -2.18049601e-01  1.56891569e-01\n",
      " -2.83530265e-01 -3.29926699e-01 -6.78404942e-02  3.50453734e-01\n",
      " -3.24131519e-01 -9.09007853e-04 -1.23354875e-01 -3.45233470e-01\n",
      " -4.52311546e-01  7.44896114e-01  1.46970570e-01 -1.25839904e-01\n",
      " -1.07294962e-01  4.01940018e-01  1.11972339e-01  2.22993977e-02\n",
      " -3.72039467e-01  2.02560142e-01  3.16281393e-02  2.91241556e-02\n",
      " -2.40586206e-01  1.36774838e-01 -1.75260063e-02  1.01980194e-01\n",
      "  8.33696201e-02  5.01191735e-01 -3.97316903e-01  4.00523953e-02\n",
      " -1.65336326e-01 -1.89155132e-01 -1.44131929e-01  6.28692061e-02\n",
      " -5.18540621e-01 -2.63796657e-01  3.16548571e-02 -6.03113696e-02\n",
      "  1.01237603e-01 -5.40754557e-01 -3.52765709e-01 -1.28057823e-01\n",
      " -2.61692256e-01 -2.60725200e-01 -9.15315449e-02  3.09405506e-01\n",
      "  4.46753263e-01 -2.52553999e-01 -2.84168422e-01 -9.30303395e-01\n",
      " -3.27120796e-02 -4.66910541e-01  4.06448603e-01  2.04477414e-01\n",
      "  2.22286612e-01  2.50097722e-01 -4.57673043e-01  4.08855408e-01\n",
      "  1.26054496e-01 -2.00009704e-01 -8.70249942e-02 -2.70096391e-01\n",
      "  3.45325351e-01  7.20757395e-02  1.27733245e-01 -2.56741885e-02\n",
      "  2.31918097e-01  3.71200472e-01  5.39735258e-02  1.50234312e-01\n",
      "  2.97337472e-01  7.63115808e-02 -8.79110396e-02  2.38627642e-01\n",
      "  3.56182575e-01 -6.44256771e-02  2.08609164e-01 -3.86539042e-01\n",
      " -1.94404759e-02  5.60759604e-01 -6.43926084e-01  2.10351840e-01]\n"
     ]
    }
   ],
   "source": [
    "print('Information stored in the vocabulary for the word \"man\":')\n",
    "man_vector=wiki2vec.get_word_vector('man')\n",
    "print(type(man_vector))\n",
    "print('Distributional meaning of \"man\" in Wikipedia:', man_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, a vector is a sorted bunch of numbers, each representing a dimension. These numbers are actually the weights learned by the neural network that are applied to the hidden layer when learning to predict the context words of 'man'.\n",
    "\n",
    "How many do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vector dimensions: 100\n"
     ]
    }
   ],
   "source": [
    "print('Number of vector dimensions:', len(man_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a surprise: we loaded a model with 100 dimensions based on a hidden layer with 100 neurons. This is true for all words so also for 'dog'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector4dog = wiki2vec.get_word_vector('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector4dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01304934  0.64761317  0.10454331 -0.3116781   0.14754549  0.0808935\n",
      " -0.15227112  0.26503846 -0.64620554 -0.20578592  0.15636262 -0.20720603\n",
      "  0.41793343  0.03861991 -0.01935025 -0.22413553  0.22274837 -0.34524342\n",
      " -0.42599422  0.102845   -0.21360567 -0.02671032  0.19456221  0.3651903\n",
      " -0.22647302 -0.27360198  0.03258029 -0.02785098 -0.23588972  0.5077206\n",
      "  0.37592876 -0.22071666 -0.05057421  0.7909033   0.1343578  -0.07903094\n",
      " -0.4099386   0.15587732 -0.00657076  0.1236117  -0.54740536 -0.08774299\n",
      " -0.3738407  -0.25297046 -0.4688306  -0.11844479 -0.05014395  0.32674935\n",
      " -0.17993684 -0.26620498  0.09679675  0.28913295 -0.4815562  -0.3374474\n",
      "  0.24882683  0.17436764  0.0888719  -0.18725184 -0.33120757 -0.1903342\n",
      "  0.05470906 -0.61491376 -0.42699674 -0.10787722  0.13698857 -0.14450763\n",
      "  0.05210021 -0.5711191  -0.38591218 -0.6626211   0.2417067  -0.01411594\n",
      "  0.39739552  0.13306352 -0.6726368  -0.22698367  0.1793001   0.24538207\n",
      "  0.15446481 -0.09232827 -0.02473994 -0.4611105  -0.13171642 -0.21940833\n",
      "  0.21425298  0.49103034 -0.21856439  0.24628595  0.08428791  0.13236591\n",
      " -0.45648926  0.00804436  0.6242295  -0.02165581 -0.08403038 -0.47217292\n",
      "  0.11906766  0.32987368 -0.9190552   0.09625731]\n"
     ]
    }
   ],
   "source": [
    "print(vector4dog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the representations are compatible across the words, we can compare two vector representations through the cosine similarity function:\n",
    "\n",
    "![Cosine similarity](./images/cosine-full.png \"Logo Title Text 1\")\n",
    "\n",
    "So suppose we have two vectors A and B, each with 100 slots, this formula (taken from the Wikipedia page) tells you to sum the results of multiplying each slot across A and B:\n",
    "\n",
    "A[0]\\*B[0]+A[1]\\*B[1]+....A[99]\\*B[99]\n",
    "\n",
    "Next we divide this sum by the square-root of the total sum of the slots of A, multiplied by the square-root of the total sum of the slots of B. Dividing it that way normalises the value between 0 and 1 and it makes the sum of the products of the numerator relative to the product of the sums of the individual vectors.\n",
    "\n",
    "Embedding software uses such measures to obtain the most similar words. We already have the vector for 'dog' so we can now use the wiki2vec.most_similar_by_vector() function to ask for the words that are most simlar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<Word dog>, 0.9999998),\n",
       " (<Word dogs>, 0.86373067),\n",
       " (<Word cat>, 0.8286425),\n",
       " (<Word puppy>, 0.8150868),\n",
       " (<Word rabbit>, 0.804229),\n",
       " (<Word montarges>, 0.7981079),\n",
       " (<Word poodle>, 0.7949788),\n",
       " (<Word barfy>, 0.79154897),\n",
       " (<Word cockapoo>, 0.7834619),\n",
       " (<Word pekapoos>, 0.782865)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki2vec.most_similar_by_vector(vector4dog, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks good. We indeed get a list of closely related words. Note that some are not *dogs*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia2Vec also provides another way to get the most similar words directly from a word rather than from a vector of a word. This can be done using the Wikipedia2Vec *Word* object. Word objects are defined in the Wikipedia2Vec package as a special data type, (e.g. you do not find them in other packages such as Gensim). They provide the index to the vector in the matrix but also frequency stats at the token level and the document level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what a Wikipedia2Vec *Word* object looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'wikipedia2vec.dictionary.Word'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'count',\n",
       " 'doc_count',\n",
       " 'index',\n",
       " 'text']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_object_dog=wiki2vec.get_word('dog')\n",
    "print(type(word_object_dog))\n",
    "dir(word_object_dog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now directly apply the *most_similar* function without having to get the vector first but also get other information such as frequency information of the word in Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Word frequency statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token frequency: 116223\n",
      "Document frequency: 54616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(<Word dog>, 0.9999998),\n",
       " (<Word dogs>, 0.86373067),\n",
       " (<Word cat>, 0.8286425),\n",
       " (<Word puppy>, 0.8150868),\n",
       " (<Word rabbit>, 0.804229),\n",
       " (<Word montarges>, 0.7981079),\n",
       " (<Word poodle>, 0.7949788),\n",
       " (<Word barfy>, 0.79154897),\n",
       " (<Word cockapoo>, 0.7834619),\n",
       " (<Word pekapoos>, 0.782865)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Token frequency:', word_object_dog.count)\n",
    "print('Document frequency:', word_object_dog.doc_count)\n",
    "wiki2vec.most_similar(word_object_dog, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we discussed the notion of information value in the class as expressed by the 'TD\\*IDF' formulae (Term frequency times the inversed document frequency). We could easily get a basic information value score based on the Wikipedia articles by dividing the token count by the document count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information value of \"dog\" 2.1280027830672332\n",
      "Information value of \"cat\" 2.2003139717425433\n",
      "Information value of \"the\" 29.74152814006325\n",
      "Information value of \"for\" 5.936486691616065\n",
      "Information value of \"link\" 1.7776395603831898\n",
      "Information value of \"article\" 1.5689368990990131\n",
      "Information value of \"Trump\" 3.7625386706665416\n",
      "Information value of \"Poetin\" 2.25\n"
     ]
    }
   ],
   "source": [
    "print('Information value of \"dog\"', wiki2vec.get_word('dog').count/wiki2vec.get_word('dog').doc_count)\n",
    "print('Information value of \"cat\"', wiki2vec.get_word('cat').count/wiki2vec.get_word('cat').doc_count)\n",
    "\n",
    "print('Information value of \"link\"', wiki2vec.get_word('link').count/wiki2vec.get_word('link').doc_count)\n",
    "print('Information value of \"article\"', wiki2vec.get_word('article').count/wiki2vec.get_word('article').doc_count)\n",
    "\n",
    "print('Information value of \"Trump\"', wiki2vec.get_word('trump').count/wiki2vec.get_word('trump').doc_count)\n",
    "print('Information value of \"Poetin\"', wiki2vec.get_word('poetin').count/wiki2vec.get_word('poetin').doc_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that general words have lower scores than names of inviduals such as *Trump* and *Poetin*. Typical Wikipedia jargon has lowest information value as it occurs in many different articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, ```TF*IDF``` does not work out-of-the-box for very freqeunt words such as *the* and *for*. This is because their frequency is so high in comparison to the number of Wikipedia articles that we still get high values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information value of \"the\" 29.74152814006325\n",
      "Information value of \"for\" 5.936486691616065\n"
     ]
    }
   ],
   "source": [
    "print('Information value of \"the\"', wiki2vec.get_word('the').count/wiki2vec.get_word('the').doc_count)\n",
    "print('Information value of \"for\"', wiki2vec.get_word('for').count/wiki2vec.get_word('for').doc_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Wikipedia as a genre the document count needs to be boosted to work. There are may different variants of ```TD*IDF``` that adjust for peculiarities of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Entity embeddings and statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia does not only have text but also a lot of entity mentions. Wiki2Vec therefore allows you to obtain entities and entity vectors as well. The package defined another specific object type *Entity* for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'wikipedia2vec.dictionary.Entity'>\n",
      "['__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', 'count', 'doc_count', 'index', 'title']\n"
     ]
    }
   ],
   "source": [
    "scarlett_entity=wiki2vec.get_entity('Scarlett Johansson')\n",
    "print(type(scarlett_entity))\n",
    "print(dir(scarlett_entity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that *Entity* has similar attributes and functions as *Word*. There are also a few differences such as e.g. *title*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Scarlett Johansson\n",
      "Token frequency: 687\n",
      "Document frequency: 582\n",
      "[(<Entity Scarlett Johansson>, 1.0), (<Word charlize>, 0.76443684), (<Word winslet>, 0.72750795), (<Entity Hilary Swank>, 0.7215545), (<Word paltrow>, 0.71570885), (<Entity Eva Green>, 0.71551895), (<Word noomi>, 0.714082), (<Entity Kate Winslet>, 0.7103845), (<Entity Keira Knightley>, 0.7100761), (<Word blanchett>, 0.7095127)]\n"
     ]
    }
   ],
   "source": [
    "print('Title:', scarlett_entity.title)\n",
    "print('Token frequency:', scarlett_entity.count)\n",
    "print('Document frequency:', scarlett_entity.doc_count)\n",
    "\n",
    "scarlet_list = wiki2vec.most_similar(scarlett_entity, 10)\n",
    "print(scarlet_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most_similar function now gives a mixture of entities and words. This is nice property of Wikipedia2Vec because it includes the linked entities as a structure.\n",
    "\n",
    "Also note that the token frequency of 'Scarlett Johansson' is just a bit higher than the document frequency. This means she is mostly mentioned only once per article in which she occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way as for words, you can get the entity vector as well and get the most similar by vector results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(<Entity Scarlett Johansson>, 1.0),\n",
       " (<Word charlize>, 0.76443684),\n",
       " (<Word winslet>, 0.72750795),\n",
       " (<Entity Hilary Swank>, 0.7215545),\n",
       " (<Word paltrow>, 0.71570885),\n",
       " (<Entity Eva Green>, 0.71551895),\n",
       " (<Word noomi>, 0.714082),\n",
       " (<Entity Kate Winslet>, 0.7103845),\n",
       " (<Entity Keira Knightley>, 0.7100761),\n",
       " (<Word blanchett>, 0.7095127)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scarlett=wiki2vec.get_entity_vector('Scarlett Johansson')\n",
    "print(len(scarlett))\n",
    "wiki2vec.most_similar_by_vector(scarlett, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Links to existing models available for download\n",
    "\n",
    "Follow the links to browse available models. The sources listed below contain English models trained using different algorithms, data with different degrees of preprocessing and varying hyperparameter settings. Some resources also include models in other languages.\n",
    "\n",
    "### Large and commonly used models (English):\n",
    "\n",
    "* Google word2vec: can be downloaded from here (follow link in instructions): http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/\n",
    "\n",
    "* GloVe (trained on various corpora): https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "* FastText embeddings (Facebook): https://fasttext.cc/docs/en/english-vectors.html\n",
    "\n",
    "* Models with different algorithms, hyperparamters, dimensions and degrees of preprocessing (e.g. dependency parsing windows):  https://vecto.readthedocs.io/en/docs/tutorial/getting_vectors.html\n",
    "\n",
    "\n",
    "### Various models in English & other languages:\n",
    "\n",
    "* word2vec trained on Wikipedia for various languages: https://wikipedia2vec.github.io/wikipedia2vec/pretrained/\n",
    "\n",
    "* Various algorithms and parameters for English and other languages: http://vectors.nlpl.eu/repository/#\n",
    "\n",
    "* Word2vec wikipedia for English and German: https://github.com/idio/wiki2vec\n",
    "\n",
    "* Facebook's fastText (https://fasttext.cc) for languages other than English: https://fasttext.cc/docs/en/crawl-vectors.html \n",
    "\n",
    "\n",
    "Gensim even lets you download models directly via their api. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tip: You can build your own word embedding model from a text corpus. Normally, you need a very large corpus to do this but it may be beneficiary to create a dedicated word embedding model for your application. If you build your own embedding space you can visualise it in:\n",
    "    http://projector.tensorflow.org/?config=https://wikipedia2vec.github.io/projector_files/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
