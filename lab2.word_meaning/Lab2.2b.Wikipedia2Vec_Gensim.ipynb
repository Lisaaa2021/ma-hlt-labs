{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB2.2b: Word embeddings from Wikipedia using Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you cannot install Wikipedia2Vec you can try to install the *gensim* package and load the models as described below.\n",
    "This notebook explains how you can load a Wiki2Vec model in the Word2Vec text format with the Gensim model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to install Gensim on your local machine from the command line:\n",
    "\n",
    "    conda install -c conda-forge gensim\n",
    "\n",
    "If gensim is succefully installed you can use existing models Wikipedia2Vec models in a text (txt) format that is compatable with gensim.\n",
    "\n",
    "You can download pre-trained models in various languages from: https://wikipedia2vec.github.io/wikipedia2vec/pretrained/\n",
    "\n",
    "There are different variants trained for 100 and 300 dimensions. If your computer has limited capacity, it is better to start with the 100 dimensions. For this notebook, we will download enwiki_20180420_100d.txt.bz2, which is a compressed version of the 100 dimensions embeddings model built from the English Wikipedia. You need to decompress the \"txt.bz2\" file to a file with the extension \".txt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: \\ ^C\n",
      "failed with initial frozen solve. Retrying with flexible solve.\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install -c conda-forge gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the path to your local copy of an embedding model.\n",
    "# Here we specify an example of such a path. Adapt the path to where you have stored the donwload\n",
    "# Make sure it is decompressed. The *.bz2 file will not load.\n",
    "#Parameters\n",
    "#fname (str) – The file path to the saved word2vec-format file.\n",
    "\n",
    "#fvocab (str, optional) – File path to the vocabulary.Word counts are read from fvocab filename, if set (this is the file generated by -save-vocab flag of the original C tool).\n",
    "\n",
    "#binary (bool, optional) – If True, indicates whether the data is in binary word2vec format.\n",
    "\n",
    "#encoding (str, optional) – If you trained the C model using non-utf8 encoding for words, specify that encoding in encoding.\n",
    "\n",
    "#unicode_errors (str, optional) – default ‘strict’, is a string suitable to be passed as the errors argument to the unicode() (Python 2.x) or str() (Python 3.x) function. If your source file may include word tokens truncated in the middle of a multibyte unicode character (as is common from the original word2vec.c tool), ‘ignore’ or ‘replace’ may help.\n",
    "\n",
    "#limit (int, optional) – Sets a maximum number of word-vectors to read from the file. The default, None, means read all.\n",
    "\n",
    "#datatype (type, optional) – (Experimental) Can coerce dimensions to a non-default float type (such as np.float16) to save memory. Such types may result in much slower bulk operations or incompatibility with optimized routines.)\n",
    "\n",
    "#Returns\n",
    "#Loaded model.\n",
    "#Return type Word2VecKeyedVectors\n",
    "\n",
    "\n",
    "# Loading the model can take a while.\n",
    "\n",
    "MODEL_FILE='/Users/piek/Desktop/t-ONDERWIJS/data/word-embeddings/wiki2vec/enwiki_20180420_100d.txt'\n",
    "\n",
    "wiki2vec = KeyedVectors.load_word2vec_format(MODEL_FILE) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By loading the model, we created an object with the name \"wiki2vec\" through which we can call functions and attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(wiki2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models may be stored in different (and sometimes confusing) formats, but they all boil down to these components:\n",
    "\n",
    "* a matrix of word vectors \n",
    "* a vocabulary\n",
    "* a mapping between vectors in the matrix to the words in the vocabulary (often via indices)\n",
    "\n",
    "Think about what a matrix is (no not the movie). You know that a vector is a list of digits, such that each digit is a value for a dimension in an n-dimensional space. Well, if you have a list of these vectors you have a matrix of n-columns and m-rows. Each row corresponds to the vector of a word in the vocabulary.\n",
    "\n",
    "The matrix of 3 rows and 3 columns\n",
    "```\n",
    "[[.34, .56, ,12],\n",
    " [.12, .39, ,05],\n",
    " [.78, .37, ,01]]\n",
    "```\n",
    "\n",
    "The vocabulary with the word as a key and the matrix list index that points to the row with the embedding for the word:\n",
    "\n",
    "```{\"dog\": 0, \"cat\" : 1, \"car\" : 2}```\n",
    "\n",
    "For this data, a simple lookup function for *dog* will give the embedding *[.34, .56, ,12]*.\n",
    "\n",
    "Now let's see how this is implemented in GenSim for the Wikipedia derived word embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the wiki2vec model as a python object:\n",
    "print('The model is represented internally as a...')\n",
    "print(type(wiki2vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has a dictionary that contains words among others. Let's check how big the vocabulary is of the model derived from English Wikipedia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocabulary = wiki2vec.wv.dictionary\n",
    "print('The model vocabulary is represented internally as a...')\n",
    "print(type(wiki2vec.vocab))\n",
    "\n",
    "# Show some properties of our model. Notice these are also in the text file.\n",
    "print('Vector size =', wiki2vec.vector_size)\n",
    "print('Vocabulary size =', len(wiki2vec.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four-and-a-haf millions words are present in this model. That is a lot more than in the English WordNet. Let's check some of these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some words from the model vocabulary:\n",
      "['the', 'in', 'of', 'a', 'and', 'is', 'to', 'was', 'by', 'for', 'on', 'as', 'at', 'from', 'with', 'an', 'it', 'that', 'also', 'which']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#####\n",
    "print('Some words from the model vocabulary:')\n",
    "print(list(wiki2vec.vocab)[:20]) #Note that :20 gives the first 20 items in the list, print(list(vocabulary.words())[-1]) gives the last word\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word in the vocabulary, we can now get the vector. We assume that 'man' is in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information stored in the vocabulary for the word \"man\":\n",
      "<class 'numpy.ndarray'>\n",
      "Embedding of \"man\" in Wikipedia: [-2.377e-01  2.686e-01 -9.620e-02  2.707e-01 -2.241e-01 -2.489e-01\n",
      "  1.065e-01  4.120e-02 -5.349e-01 -1.445e-01 -8.700e-02 -1.877e-01\n",
      "  1.985e-01 -1.643e-01  1.021e-01 -1.783e-01 -5.520e-02  2.190e-02\n",
      " -2.180e-01  1.569e-01 -2.835e-01 -3.299e-01 -6.780e-02  3.505e-01\n",
      " -3.241e-01 -9.000e-04 -1.234e-01 -3.452e-01 -4.523e-01  7.449e-01\n",
      "  1.470e-01 -1.258e-01 -1.073e-01  4.019e-01  1.120e-01  2.230e-02\n",
      " -3.720e-01  2.026e-01  3.160e-02  2.910e-02 -2.406e-01  1.368e-01\n",
      " -1.750e-02  1.020e-01  8.340e-02  5.012e-01 -3.973e-01  4.010e-02\n",
      " -1.653e-01 -1.892e-01 -1.441e-01  6.290e-02 -5.185e-01 -2.638e-01\n",
      "  3.170e-02 -6.030e-02  1.012e-01 -5.408e-01 -3.528e-01 -1.281e-01\n",
      " -2.617e-01 -2.607e-01 -9.150e-02  3.094e-01  4.468e-01 -2.526e-01\n",
      " -2.842e-01 -9.303e-01 -3.270e-02 -4.669e-01  4.064e-01  2.045e-01\n",
      "  2.223e-01  2.501e-01 -4.577e-01  4.089e-01  1.261e-01 -2.000e-01\n",
      " -8.700e-02 -2.701e-01  3.453e-01  7.210e-02  1.277e-01 -2.570e-02\n",
      "  2.319e-01  3.712e-01  5.400e-02  1.502e-01  2.973e-01  7.630e-02\n",
      " -8.790e-02  2.386e-01  3.562e-01 -6.440e-02  2.086e-01 -3.865e-01\n",
      " -1.940e-02  5.608e-01 -6.439e-01  2.104e-01]\n"
     ]
    }
   ],
   "source": [
    "print('Information stored in the vocabulary for the word \"man\":')\n",
    "man_vector=wiki2vec['man']\n",
    "print(type(man_vector))\n",
    "print('Embedding of \"man\" in Wikipedia:', man_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, a vector is a sorted bunch of numbers, each representing a dimension. These numbers are actually the weights learned by the neural network that are applied to the hidden layer when learning to predict the context words of 'man'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data object is of the type 'numpy.ndarray'. Numpy is a package for dealing with numerical data that is used a lot in machine learning. For those interested, here is the description of what it is:\n",
    "\n",
    "https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many dimensions do we have in this vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy data shape (100,)\n",
      "Number of vector dimensions: 100\n"
     ]
    }
   ],
   "source": [
    "print('Numpy data shape', man_vector.shape)\n",
    "print('Number of vector dimensions:', len(man_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a surprise: we loaded a model with 100 dimensions based on a hidden layer with 100 neurons. This is true for all words so also for 'dog'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector4dog = wiki2vec['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector4dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.013   0.6476  0.1045 -0.3117  0.1475  0.0809 -0.1523  0.265  -0.6462\n",
      " -0.2058  0.1564 -0.2072  0.4179  0.0386 -0.0194 -0.2241  0.2227 -0.3452\n",
      " -0.426   0.1028 -0.2136 -0.0267  0.1946  0.3652 -0.2265 -0.2736  0.0326\n",
      " -0.0279 -0.2359  0.5077  0.3759 -0.2207 -0.0506  0.7909  0.1344 -0.079\n",
      " -0.4099  0.1559 -0.0066  0.1236 -0.5474 -0.0877 -0.3738 -0.253  -0.4688\n",
      " -0.1184 -0.0501  0.3267 -0.1799 -0.2662  0.0968  0.2891 -0.4816 -0.3374\n",
      "  0.2488  0.1744  0.0889 -0.1873 -0.3312 -0.1903  0.0547 -0.6149 -0.427\n",
      " -0.1079  0.137  -0.1445  0.0521 -0.5711 -0.3859 -0.6626  0.2417 -0.0141\n",
      "  0.3974  0.1331 -0.6726 -0.227   0.1793  0.2454  0.1545 -0.0923 -0.0247\n",
      " -0.4611 -0.1317 -0.2194  0.2143  0.491  -0.2186  0.2463  0.0843  0.1324\n",
      " -0.4565  0.008   0.6242 -0.0217 -0.084  -0.4722  0.1191  0.3299 -0.9191\n",
      "  0.0963]\n"
     ]
    }
   ],
   "source": [
    "print(vector4dog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the representations are compatible across the words, we can compare two vector representations through the cosine similarity function:\n",
    "\n",
    "![Cosine similarity](./images/cosine-full.png \"Logo Title Text 1\")\n",
    "\n",
    "So suppose we have two vectors A and B, each with 100 slots, this formula (taken from the Wikipedia page) tells you to sum the results of multiplying each slot across A and B:\n",
    "\n",
    "A[0]\\*B[0]+A[1]\\*B[1]+....A[99]\\*B[99]\n",
    "\n",
    "We divide this sum by the square-root of the total sum of the slots of A, multiplied by the square-root of the total sum of the slots of B. Dividing it that way normalises the value between 0 and 1 and it makes the sum of the products of the numerator relative to the product of the sums of the individual vectors.\n",
    "\n",
    "Embedding software uses such measures to obtain the most similar words. We can now use the *most_similar_by_word* function from Gensim to ask for the words that are most similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_sim = wiki2vec.similar_by_word(\"dog\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is imporant to note that Gensim is not the same package as Wikipedia2Vec. Gensim is more generic and does not have the specific data objects of Wikipedia2Vec for the frequency of words or the entities in the Wikipedia articles. However, you can also see that the other notebook for Wikipedia2Vec showed the same results for 'dog'. The way the model is loaded and treated is therefore the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[(<Word dog>, 0.99999994),\n",
    " (<Word dogs>, 0.8637307),\n",
    " (<Word cat>, 0.8286426),\n",
    " (<Word puppy>, 0.81508684),\n",
    " (<Word rabbit>, 0.8042291),\n",
    " (<Word montarges>, 0.798108),\n",
    " (<Word poodle>, 0.79497886),\n",
    " (<Word barfy>, 0.7915491),\n",
    " (<Word cockapoo>, 0.783462),\n",
    " (<Word pekapoos>, 0.78286505)]\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7570435\n",
      "0.25811055\n"
     ]
    }
   ],
   "source": [
    "print(wiki2vec.similarity(\"king\", \"queen\"))\n",
    "print(wiki2vec.similarity(\"king\", \"coffee\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
