{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3.6 Training an emotion classifier using word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a follow up on the notebook: Lab3.3a.ml.emotion-detection.ipynb and on Lab2. \n",
    "\n",
    "The previous notebook *Lab3.3a.ml.emotion-detection* showed how you can create a bag-of-words vector representation from a text and how to feed it to a machine to learn to assign emotion labels to it. Words from the training set become features in the vector and the machine learns to associate these with the emotion classes. Unseen text is represented in the same way as a bag-of-words vector and through similarity with the training data, the machine makes a prediction on the emotion encoded in the unseen text.\n",
    "\n",
    "In this notebook we are going to replace the bag-of-words by a word embedding representation. We discussed word-embeddings in Lab2 as a powerful model to represent families of words that are related. Now imagine that we replace our text representations of words by a representation based on embeddings? Our hypothesis is that training data will be boosted to represent families of related words and can be compared to unseen data with words belonging to similar families but not literally. Our hypothesis is that this will improve the recall but maybe at the price of precision. Our evaluation will show if this is the case.\n",
    "\n",
    "Using embeddings to replace word tokens in data, is a powerful method to make training data more robust. Whereas the vectors for tokens are large and sparse (thousands of dimensions and many zero values), the embeddings representaions are small and dense (300 dimensions or less and all dimensions have some positive or negative value).\n",
    "\n",
    "We will test this hypothesis in this notebook by creating vectors based on *averaged* word vectors for the same MELD data and testing it in the same way as we did before on MELD data and our own set of utterances.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "* [Section 1: Quick introduction to embeddings](#section1)\n",
    "* [Section 2: Loading the emotion data](#section2)\n",
    "* [Section 3: Preparing the training and test data](#section3)\n",
    "* [Section 4: Training and applying the model](#section4)\n",
    "* [Section 5: Generating the test report](#section5)\n",
    "* [Section 6: Applying the classifier to your own text](#section6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Quick introduction to embeddings  <a class=\"anchor\" id =\"section1\"></a> \n",
    "\n",
    "A recent alternative way to create a 'semantic' representation of a word is by word embeddings: mapping words (or phrases) from the vocabulary to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with many dimensions per word to a continuous vector space with a much lower dimension. For this reason, they are called dense representations.\n",
    "\n",
    "In linguistics, word embeddings were discussed in the research area of distributional semantics. The idea is to quantify and categorize semantic similarities between linguistic items based on their distributional properties in large samples of language data. The underlying notion is that \"a word is characterized by the company it keeps\" (Firth). Embeddings do not directly represent these context words but are the learned weights in the hidden layer of a neural network that tries to predict the context words. In that sense, we do not need thousands of dimensions to represent all possible context words but just the learned weights.\n",
    "\n",
    "### Reference:\n",
    "\n",
    "For another explanation how word embedddings can improve classical bag-of-word approaches, check out this page:\n",
    "\n",
    "https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in Lab2, we are going to use the *gensim* package to load and use prebuilt embdiings models. Assuming it is already installed on your machine, we first import *gensim*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many sites that provide pretrained word2vec models that can be loaded through the *gensim* package. Check out the original data from Google for a word2vec model with 300 dimensions trained from Wikipedia and news: [Google code archive](https://code.google.com/archive/p/word2vec/). Here is another website with many ready to use models: http://vectors.nlpl.eu/repository/\n",
    "\n",
    "Whatever you choose, make sure you can load the model using the 'gensim' package.\n",
    "\n",
    "In this notebook, we will load pre-trained word embeddings, created in the [Glove project](https://nlp.stanford.edu/projects/glove/) from Stanford University. We will use embeddings trained from twitter data. We hope that twitter model is more adapted to the spoken utterances from the MELD project than other Google and Glove models trained on written news and Wikipedia articles.\n",
    "\n",
    "We will load the model with the Gensim package that we used before but you can also use the *gensim* api to load the model online, assuming you have a good network connection. If you want to download and load it from disk, follow the instructions for the next cell. If you do not want to store it on disk but load it online, skip to the next subsection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Downloading models and loading from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the twitter models to your disk from:\n",
    "\n",
    "http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "\n",
    "You can see that different models are provided with different dimensions:\n",
    "\n",
    "Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors, 1.42 GB download): glove.twitter.27B.zip\n",
    "\n",
    "On the Stanford website, also other models are provided: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "We will use the model with 200 dimensions. If your computer cannot handle it, you can try one of the smaller models.\n",
    "\n",
    "When you unpack the zip file, you see that the models are provided as text files. To load the data into *gensim*, we need to carry some specific code given in the next cell. Don't worry too much about this code. Adapt the path to your local copy and run the cell to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "### Adapt the path to your local copy\n",
    "wordembeddings=\"glove.twitter.27B.200d.txt\"\n",
    "glove_file = datapath('/Users/piek/Desktop/t-ONDERWIJS/data/word-embeddings/classical-models/glove-twitter-models/glove.twitter.27B.200d.txt')\n",
    "tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
    "\n",
    "_ = glove2word2vec(glove_file, tmp_file)\n",
    "word_embedding_model = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "\n",
    "# ### this model has 200 dimensions so we set the number of features to 200\n",
    "num_features = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Loading models using the gensim API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of downloading the models to disk, 'gensim' also provides a downloader API to load the model from the web when needed. In the next cell, we use this API to download a word embeddding model trained on tweets. The following cell use the gensim API. Note that it takes some time to download but it saves some disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "# wordembeddings = \"glove-twitter-200\"\n",
    "# ### this model has 200 dimensions so we set the number of features to 200\n",
    "# num_features = 200\n",
    "\n",
    "wordembeddings = \"glove-twitter-25\"\n",
    "### this model has 25 dimensions so we set the number of features to 25\n",
    "num_features = 25\n",
    "\n",
    "#wordembeddings = \"glove-twitter-50\"\n",
    "### this model has 50 dimensions so we set the number of features to 50\n",
    "#num_features = 50\n",
    "\n",
    "#wordembeddings = \"glove-twitter-100\"\n",
    "### this model has 100 dimensions so we set the number of features to 100\n",
    "#num_features = 100\n",
    "\n",
    "#wordembeddings = \"glove-wiki-gigaword-300\"\n",
    "#num_features = 300\n",
    "\n",
    "word_embedding_model = api.load(wordembeddings)\n",
    "print(num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the word embedding model and training the classifier may take a while. If your laptop cannot handle this, use a smaller word embeddings model with less dimensions. Note that the performance of the classifier may be degraded when fewer dimensions are used. Alternatively, you can reduced the number of training data as we will show below but this will also likely affect the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the embedding model you have selected, you need to set the number of features that are used to create vectors for the utterances equal to the number of dimensions. If the value of num_features is different from the dimensions of the model, you get an error creating the embedding representations for the utterances below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9590821]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "word1='cat'\n",
    "word2='dog'\n",
    "word1_vector=np.array(word_embedding_model[word1]).reshape(1, -1)\n",
    "word2_vector=np.array(word_embedding_model[word2]).reshape(1, -1)\n",
    "print(cosine_similarity(word1_vector, word2_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the emotion data  <a class=\"anchor\" id =\"section2\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with the previous notebook, we load the training and test data from the MELD data set. This code is the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/.local/lib/python3.7/site-packages/ipykernel_launcher.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \n",
      "/Users/piek/.local/lib/python3.7/site-packages/ipykernel_launcher.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "filepath = './data/MELD/train_sent_emo.csv'\n",
    "dftrain = pd.read_csv(filepath)\n",
    "### The data has some problematic strings with encoding problems. The next code removes some of these from the utterances\n",
    "# Try to fix encoding\n",
    "dftrain['Utterance'] = dftrain['Utterance'].str.replace(\"\\x92|\\x97|\\x91|\\x93|\\x94|\\x85\", \"'\")\n",
    "\n",
    "filepath = './data/MELD/test_sent_emo.csv'\n",
    "dftest = pd.read_csv(filepath)\n",
    "dftest['Utterance'] = dftest['Utterance'].str.replace(\"\\x92|\\x97|\\x91|\\x93|\\x94|\\x85\", \"'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(text):\n",
    "    ### the first loop gets the utterances\n",
    "    text_tokens = []\n",
    "    for utterance in text:\n",
    "        text_tokens.append(nltk.tokenize.word_tokenize(utterance))\n",
    "        \n",
    "    return text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### the first loop gets the utterances\n",
    "training_instances = tokenize_data(dftrain['Utterance'])\n",
    "\n",
    "### We use the same loop for the list of emotion labels that correspond with the vector representations of each utterance\n",
    "training_labels = tokenize_data(dftrain['Emotion'])\n",
    "    \n",
    "### the first loop gets the utterances\n",
    "test_instances = tokenize_data(dftest['Utterance'])\n",
    "\n",
    "### We use the same loop for the list of emotion labels that correspond with the vector representations of each utterance\n",
    "test_labels = tokenize_data(dftest['Emotion'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preparing the training and test data  <a class=\"anchor\" id =\"section3\"></a> \n",
    "\n",
    "The following imports are needed again to create a classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, we used CountVectorizer to obtain the full vocabulary of the data set and generate vectors for the bag-of-words endocing of each word. In these vectors, each slot represents a word and a value '1' indicates that the word was present in the utterance and a '0' means absence. This results in large and sparse vector representations for each utterance. We have also seen that we can weight the relevance of a word using the 'TF.IDF' function. This still results in large and sparse vectors but weights are more subtle. The down side is sparseness, lack of generalisation and lack of robustness to deal with new unseen data.\n",
    "\n",
    "In the following, we are going to represent the utterances by an embedding representation. In fact, we take the word embedding of each token in the utterance and add these together, after which we take the average. All the embeddings have the same number of dimensions in the same order. So if two tokens have a high weight for one dimension then their co-uccurrence in an utterance will enforce that weight. Note that by adding and taking the average, we normalize for the length of the utterance and the order of the tokens is not relevant.\n",
    "\n",
    "Before we create the embedding representations for the utterances, we are going to filter  words using the NLTK stopword list and their frequency to make the text representations more compatible with our previous notebook. In the case of CountVectorizer, this was done for us using the parameter settings. \n",
    "\n",
    "In this case, we need to make our own customized function. The next piece of code shows a loop over all utterances from which we extract a list of all tokens. Next, we count these tokens and extract a list of words that occur above our frequency threshold, which we should set in the same way as for CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### This code creates a list of words above the preset frequency threshold\n",
    "\n",
    "from collections import Counter\n",
    "frequency_threshold = 4\n",
    "frequent_keywords = []\n",
    "\n",
    "####  We first will collect all tokens from all the utterances from the training data using the NLTL tokenizer\n",
    "alltokens = []\n",
    "for utterance in dftrain['Utterance']:\n",
    "    tokenlist = nltk.tokenize.word_tokenize(utterance)\n",
    "    for token in tokenlist:\n",
    "        alltokens.append(token)\n",
    "        \n",
    "#### The Counter function will create a frequency count of all the items. The result is a dictionary       \n",
    "kw_counter = Counter(alltokens)\n",
    "\n",
    "#### We now loop over the dictionary with counts to get the word and the frequency value\n",
    "for word, count in kw_counter.items():\n",
    "    if count>frequency_threshold:\n",
    "        frequent_keywords.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to lookup every word in our utterance in our word embedding model to obtain the vector representation of that word. We then take the average over all the tokens of an average by summing the weights and dividing them by the number of words.\n",
    "\n",
    "Before we lookup a word in the embedding model, we check if it belongs to the frequent word list and it is not a stopword. Otherwise, we ignore the word. \n",
    "\n",
    "We also ignore words that are not in the embedding model. So it matters what type of text the model is trained on: news, Wikipedia, Twitter, spoken dialogues. To know what the coverage is of the model for our utterances in the MELD data set, we create a list of all unknown words and all known words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to define two customized function using 'def' to create an embedding representation for each utterance. \n",
    "These functions are based on: https://www.kaggle.com/varun08/sentiment-analysis-using-word2vec\n",
    "and adapted for our purposes.\n",
    "\n",
    "The first function, called 'featureVecMethod', takes the words of the utterance and the embedding model as well as some other values as parameters. The num_features parameter determines the size of the vector. If num_features is different from the actual number of dimensions of the word embedding model, this will generate an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_words =[]\n",
    "known_words = []\n",
    "\n",
    "# Function to average all word vectors in a paragraph\n",
    "def featureVecMethod(words, # Tokenized list of tokens from an utterance\n",
    "                     frequent_keywords, # List of words above the frequency threshold\n",
    "                     stop_words, # Stopwords that should be skipped\n",
    "                     model, # The actual word embeddings model\n",
    "                     modelword_index, # An index on the vocabulary of the model to speed up lookup\n",
    "                     num_features # the number of dimensions of the embedding model\n",
    "                    ):\n",
    "    # Pre-initialising empty numpy array (np) for speed\n",
    "    # This create a numpy array with the length of the num_features set to zero values\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    \n",
    "    ## A counter for the number of tokens represented so that we can take the average\n",
    "    nwords = 0\n",
    "        \n",
    "    for word in  words:\n",
    "        #### we only use words that are frequent and not stopwords\n",
    "        if word in frequent_keywords and not word in stop_words:         \n",
    "            if word in modelword_index:\n",
    "                ## The next function would directly take the values from the model\n",
    "                #featureVec = np.add(featureVec,model[word])\n",
    "                \n",
    "                ### Instead of simply taking the embedding values we prefer the next function that \n",
    "                ### normalises these values between [-1, 1] to make the average work better\n",
    "                ### Don't worry about the specifics \n",
    "                featureVec = np.add(featureVec,model[word]/np.linalg.norm(model[word]))\n",
    "\n",
    "                #we keep track of the words detected, just for analysing the data\n",
    "                known_words.append(word)\n",
    "                nwords = nwords + 1\n",
    "            else:\n",
    "                word = word.lower()\n",
    "                if word in modelword_index:\n",
    "                    #featureVec = np.add(featureVec,model[word])\n",
    "                    featureVec = np.add(featureVec,model[word]/np.linalg.norm(model[word]))\n",
    "                    \n",
    "                    #we keep track of the words detected, just for analysing the data\n",
    "                    known_words.append(word)\n",
    "                    nwords = nwords + 1\n",
    "                else:\n",
    "                    #we keep track of the unknown words to see how well our model fits the data\n",
    "                    unknown_words.append(word)\n",
    "                    \n",
    "    # Dividing the result by number of words in each utterance to get average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function just deals with all the data and creates the list of input vectors. This function calls the previous function and needs to the same parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating the average feature vector\n",
    "def getAvgFeatureVecs(texts, ### List of texts, in our case tokenized utterances\n",
    "                      keywords, \n",
    "                      stopwords, \n",
    "                      model, \n",
    "                      modelword_index, \n",
    "                      num_features\n",
    "                     ):\n",
    "    counter = 0\n",
    "    #### we initialise a numpy vector with zeros of the type float32\n",
    "    textFeatureVecs = np.zeros((len(texts),num_features),dtype=\"float32\")\n",
    "    \n",
    "    #### We iterate over all the texts\n",
    "    for text in texts:\n",
    "        # Printing a status message every 1000th text, to see what we are processing\n",
    "        if counter%1000 == 0:\n",
    "            print(\"Review %d of %d\"%(counter,len(texts)))\n",
    "        ### Each text is transformed into a vector representation based on the averaged token embedding using the previous function\n",
    "        ### We add these vectors to the total list\n",
    "        textFeatureVecs[counter] = featureVecMethod(text, keywords, stopwords, model, modelword_index,num_features)\n",
    "        counter = counter+1\n",
    "        \n",
    "    #### Due to the averaging, there could be infinitive values or NaN values. The next numpy function turns these value to \"0\" scores\n",
    "    textFeatureVecs = np.nan_to_num(textFeatureVecs) \n",
    "    \n",
    "    return textFeatureVecs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now back to our input data. We iterate over the Pandas frame in the same way as before but now we extract for each utterance the embedding representation, using the above two functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/.local/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "#Allows for quicker lookup if the words exist\n",
    "index2word_set = set(word_embedding_model.wv.index2word)\n",
    "# We take the stopwords from NLTK\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 9989\n",
      "Review 1000 of 9989\n",
      "Review 2000 of 9989\n",
      "Review 3000 of 9989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/.local/lib/python3.7/site-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 4000 of 9989\n",
      "Review 5000 of 9989\n",
      "Review 6000 of 9989\n",
      "Review 7000 of 9989\n",
      "Review 8000 of 9989\n",
      "Review 9000 of 9989\n"
     ]
    }
   ],
   "source": [
    "# Calculating average feature vector for training set\n",
    "\n",
    "### The full list of utterances is passed to our customized function, with the frequent_keywords list, the stopwords, the embedding model, \n",
    "### the index and the number of features that indicates the dimensions of the model\n",
    "trainDataVecs = getAvgFeatureVecs(training_instances, frequent_keywords, stop_words, word_embedding_model, index2word_set, num_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect our training data a bit more. Depending on the break set for loading the training data, you will have a list of vectors with according length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9989"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainDataVecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the first element in the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector length 25\n",
      "[ 0.02120117 -0.00657379 -0.06321924 -0.06114222  0.04526315  0.01968975\n",
      "  0.16843882 -0.02519162  0.00887054  0.04470573  0.01918844  0.04332738\n",
      " -0.8251243   0.02614895  0.03693727  0.01248745  0.05060307 -0.00214538\n",
      " -0.04361862 -0.07821645  0.0438555  -0.03021584 -0.04616287  0.01649406\n",
      " -0.06064579]\n"
     ]
    }
   ],
   "source": [
    "print('Vector length', len(trainDataVecs[0]))\n",
    "print (trainDataVecs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is simply a list with digits, each representing the averaged weight of the tokens or words that made up the utterance. We can checks the length, which should be '300', '100', '50' or '25', etc. depending on the number of dimensions of the word2vec model that you used.\n",
    "\n",
    "There are two major differences with the bag-of-tokens that we used in the previous notebook:\n",
    "\n",
    "1. the vectors are short\n",
    "2. there are no zero's \n",
    "\n",
    "Instead of *large sparse* vectors, we now have *short dense* vectors representing each utterance. Whereas in the previous representation, each slot in the vector corresponds with a token, now each slot is a weight from the hidden layer to learn to predict others words in the context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is true for each utterance, each having a unique set of values for the same hidden layer weights. These weights now represent the meaning of the utterance for a machine, which can use a similarity function such as cosine similairty to measure the degree of equivalence across these representations. When we inspects any other utterance, we see it is represented in a simlar way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "[ 0.00523067 -0.06953822 -0.04871736 -0.01742447 -0.02224099  0.1471257\n",
      "  0.26643133  0.13727108 -0.09906736  0.04759314 -0.04575064 -0.00662527\n",
      " -0.7751947   0.04321977  0.0468939  -0.10642495  0.01143208 -0.07899665\n",
      " -0.1055036  -0.08183447  0.00405881  0.01559638 -0.12867391  0.12725593\n",
      " -0.00202912]\n"
     ]
    }
   ],
   "source": [
    "print(len(trainDataVecs[1000]))\n",
    "print(trainDataVecs[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the vectors are compatible, we can compare them in the same way as we did before for the word2vec embeddings of *cat* and *dog*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9131651]]\n"
     ]
    }
   ],
   "source": [
    "word1_vector=np.array(trainDataVecs[0]).reshape(1, -1)\n",
    "word2_vector=np.array(trainDataVecs[1000]).reshape(1, -1)\n",
    "print(cosine_similarity(word1_vector, word2_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, we use the same labels as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral'] ['joy']\n"
     ]
    }
   ],
   "source": [
    "print(training_labels[0], training_labels[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a numeric representation of each text, based on the embeddings of the words. We feed this to a classifier in the same way as we did in the previous notebooks with the Countvectorizer output.\n",
    "\n",
    "Before we can train the classifier, we  want to convert the labels to numeric values as we did before, so that we can use the evaluation report functions. Note that this is not necessary to train and use the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do that, it may be good to check which words are not in the embedding model and therefore do not contribute to the representation of the utterance. In the above function, we kept track of the unknown words. Now we can inspect this list. We use the *Counter* function to get a frequency count of these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data vocabulary statistics:\n",
      "Frequency threshold 4\n",
      "Proportion of unknown tokens 0.013026305984052463\n",
      "Number of unknown words 25\n",
      "Number of unknown word tokens: 874\n",
      "Unknown words counts\n",
      "Counter({'...': 349, \"y'know\": 320, 'and-and': 26, 'no-no-no': 18, \"i'm-i\": 18, 'hey-hey': 14, 'that-that': 12, 'we-we': 10, '10': 9, 'no-no-no-no': 8, '2': 8, '8': 8, '30': 7, '..': 7, 'yeah-yeah': 7, \"'kay\": 6, \"it's-it\": 6, 'oh-ho': 6, \"that's-that\": 5, '25': 5, 'um-hmm': 5, '40': 5, '7': 5, 'the-the': 5, 'heldi': 5})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print('Training data vocabulary statistics:')\n",
    "print('Frequency threshold', frequency_threshold)\n",
    "unknown_words_count = Counter(unknown_words)\n",
    "print('Proportion of unknown tokens', len(unknown_words)/(len(unknown_words)+len(known_words)))\n",
    "print('Number of unknown words',len(unknown_words_count))\n",
    "print('Number of unknown word tokens:', len(unknown_words))\n",
    "print('Unknown words counts')\n",
    "print(unknown_words_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the proportion of unknown words is low. Only a few words making up a small proportion of tokens could not be represented. But(!!) note that we considered only words above the frequency threshold. \n",
    "\n",
    "If you happen to use the original Google Word2Vec model, this can be about 35% of all the tokens. You can see that using different models, gives different results based on the genre match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also kept track of the *known* words, so lets check these as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of known words 1111\n",
      "Number of known words tokens: 66221\n"
     ]
    }
   ],
   "source": [
    "known_words_count = Counter(known_words)\n",
    "print('Number of known words',len(known_words_count))\n",
    "print('Number of known words tokens:', len(known_words))\n",
    "#print(known_words_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms that a decent amount of words and tokens is represented. For comparison, all tokens are represented in the case of the Bag-of-Words representation of tokens. The Bag-of-Words token representation is less semantic and has the risk of overfitting on the training data. The embedding representation is more semantic but less tuned to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we represent the test data with the same methods to make it compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 2610\n",
      "Review 1000 of 2610\n",
      "Review 2000 of 2610\n",
      "Test data vocabulary statistics:\n",
      "Frequency threshold 4\n",
      "Proportion of unknown tokens 0.012806264935364063\n",
      "Number of unknown words 25\n",
      "Number of unknown word tokens: 2224\n",
      "Unknown words counts\n",
      "Counter({\"y'know\": 980, '...': 889, 'and-and': 38, 'no-no-no': 36, '30': 31, \"i'm-i\": 30, 'hey-hey': 26, 'we-we': 22, '8': 20, \"'kay\": 18, 'the-the': 17, '10': 15, 'no-no-no-no': 14, '..': 13, 'that-that': 12, 'oh-ho': 12, '2': 8, 'yeah-yeah': 7, \"it's-it\": 6, \"that's-that\": 5, '25': 5, 'um-hmm': 5, '40': 5, '7': 5, 'heldi': 5})\n",
      "Number of known words 1111\n",
      "Number of known words tokens: 171441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/.local/lib/python3.7/site-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "testDataVecs = getAvgFeatureVecs(test_instances, frequent_keywords, stop_words, word_embedding_model, index2word_set, num_features) \n",
    "\n",
    "print('Test data vocabulary statistics:')\n",
    "print('Frequency threshold', frequency_threshold)\n",
    "\n",
    "\n",
    "unknown_words_count = Counter(unknown_words)\n",
    "print('Proportion of unknown tokens', len(unknown_words)/(len(unknown_words)+len(known_words)))\n",
    "print('Number of unknown words',len(unknown_words_count))\n",
    "print('Number of unknown word tokens:', len(unknown_words))\n",
    "print('Unknown words counts')\n",
    "print(unknown_words_count)\n",
    "\n",
    "known_words_count = Counter(known_words)\n",
    "print('Number of known words',len(known_words_count))\n",
    "print('Number of known words tokens:', len(known_words))\n",
    "#print(known_words_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as in the previous notebook, we turn the labels into numerical values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n",
      "Train labels [4, 4, 4, 4, 6, 4, 4, 4, 4, 4, 2, 4, 6, 4, 6, 5, 6, 2, 4, 4]\n",
      "Test labels [6, 0, 4, 4, 3, 3, 3, 3, 3, 3, 3, 4, 4, 5, 6, 0, 0, 0, 3, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "# first we instantiate a label encode\n",
    "le = preprocessing.LabelEncoder()\n",
    "# we fee this encoder with the complete list of labels from our data\n",
    "le.fit(training_labels+test_labels)\n",
    "print(list(le.classes_))\n",
    "training_classes = le.transform(training_labels)\n",
    "print('Train labels', list(training_classes[0:20]))\n",
    "\n",
    "test_classes = le.transform(test_labels)\n",
    "print('Test labels', list(test_classes[0:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next steps are the same as for the previous notebook, except that we pass the embedding representations of the training data.\n",
    "\n",
    "It is nice to see that you can feed these machines anything represented as numeric vectors, no matter how they are derived of created. You could even type in these numbers your self. Will take a whole though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and applying the model  <a class=\"anchor\" id =\"section4\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(max_iter=2000)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# We choose a Linear model\n",
    "svm_linear_clf = svm.LinearSVC(max_iter=2000)\n",
    "### we train the classifier through the *fit* function and by passing the training vectors and the training labels as paramters:\n",
    "svm_linear_clf.fit(trainDataVecs, training_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results, find macro recall\n",
    "y_pred_svm_linear = svm_linear_clf.predict(testDataVecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generating the test report  <a class=\"anchor\" id =\"section5\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger' 'disgust' 'fear' 'joy' 'neutral' 'sadness' 'surprise']\n",
      "Embeddings SVM LINEAR ----------------------------------------------------------------\n",
      "Word embedding model used glove-twitter-25\n",
      "Word frequency threshold 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0  0.4146341 0.0492754 0.0880829       345\n",
      "           1  0.0000000 0.0000000 0.0000000        68\n",
      "           2  0.0000000 0.0000000 0.0000000        50\n",
      "           3  0.4406250 0.3507463 0.3905817       402\n",
      "           4  0.5548837 0.9498408 0.7005285      1256\n",
      "           5  0.0000000 0.0000000 0.0000000       208\n",
      "           6  0.5050505 0.1779359 0.2631579       281\n",
      "\n",
      "    accuracy                      0.5367816      2610\n",
      "   macro avg  0.2735991 0.2182569 0.2060501      2610\n",
      "weighted avg  0.4440740 0.5367816 0.4372466      2610\n",
      "\n",
      "Confusion matrix\n",
      "[[  17    0    0   70  242    0   16]\n",
      " [   0    0    0    7   60    0    1]\n",
      " [   1    0    0    7   41    0    1]\n",
      " [   5    0    0  141  250    0    6]\n",
      " [   9    0    0   30 1193    0   24]\n",
      " [   4    0    0   10  193    0    1]\n",
      " [   5    0    0   55  171    0   50]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#### this report gives the results for the LINEAR classifier\n",
    "report = classification_report(test_classes,y_pred_svm_linear,digits = 7)\n",
    "print(le.classes_)\n",
    "print('Embeddings SVM LINEAR ----------------------------------------------------------------')\n",
    "print('Word embedding model used', wordembeddings)\n",
    "print('Word frequency threshold', frequency_threshold)\n",
    "print(report)\n",
    "print('Confusion matrix')\n",
    "print(sklearn.metrics.confusion_matrix(test_classes,y_pred_svm_linear))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the results from the notebook where we trained a NaiveBayes and SVM classifiers with one-hot-encodings of the words? Take some time to compare the results and think about the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about our hypothesis. We see that it is not confirmed by this experiment. Results are slightly less than the BoW approach.\n",
    "\n",
    "Can you think of possible explanations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Applying the model to new data  <a class=\"anchor\" id =\"section6\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to apply the embedding based model to our own data but this works a bit different as we cannot simply use the 'transform' function to represent the utterances using the one-hot vector representation of the training vocabulary.\n",
    "\n",
    "What we need to do is to create an embedding representation using the same function we used above and assume that our classifier finds sufficient similarity in the embeddings of our data with the correct training data.\n",
    "\n",
    "We use the same set of utterances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some utterances\n",
    "some_chat = ['That is sweet of you', \n",
    "               'You are so funny', \n",
    "               'Are you a man or a woman?', \n",
    "               'Chatbots make me sad and feel lonely.', \n",
    "               'Your are stupid and boring.', \n",
    "               'Two thumbs up', \n",
    "               'I fell asleep halfway through this conversation', \n",
    "               'Wow, I am really amazed.', \n",
    "               'You are amazing.',\n",
    "             'I feel so low being in isolation',\n",
    "             'People dumping waste are horrible',\n",
    "             'Its awful that you cannot stop smoking',\n",
    "             'Dogs scare me',\n",
    "             'I am afraid I will get sick at work',\n",
    "             'I run away when I see a dog',\n",
    "             'When do you start your job?'\n",
    "            ]\n",
    "\n",
    "\n",
    "len(some_chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the list of labels that go with our chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_chat_emotions = ['joy', 'joy', 'neutral', 'sadness', 'anger', 'joy', 'anger', 'surprise', 'joy', 'sadness', 'disgust', 'disgust', 'fear', 'fear', 'fear', 'neutral']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We  use the LabelEncoder *le* to convert this list into a numpy array with digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels ['anger' 'disgust' 'fear' 'joy' 'neutral' 'sadness' 'surprise']\n",
      "[3 3 4 5 0 3 0 6 3 5 1 1 2 2 2 4]\n"
     ]
    }
   ],
   "source": [
    "print('labels',le.classes_)\n",
    "some_chat_labels = le.transform(some_chat_emotions)\n",
    "print(some_chat_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/.local/lib/python3.7/site-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "some_chat_tokens = tokenize_data(some_chat)\n",
    "some_chat_embedding_vectors = getAvgFeatureVecs(some_chat_tokens, frequent_keywords,stop_words, word_embedding_model, index2word_set, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System predictions [3 6 4 4 4 4 4 4 3 4 4 4 4 4 4 4]\n",
      "Gold labels [3 3 4 5 0 3 0 6 3 5 1 1 2 2 2 4]\n",
      "That is sweet of you => joy\n",
      "You are so funny => surprise\n",
      "Are you a man or a woman? => neutral\n",
      "Chatbots make me sad and feel lonely. => neutral\n",
      "Your are stupid and boring. => neutral\n",
      "Two thumbs up => neutral\n",
      "I fell asleep halfway through this conversation => neutral\n",
      "Wow, I am really amazed. => neutral\n",
      "You are amazing. => joy\n",
      "I feel so low being in isolation => neutral\n",
      "People dumping waste are horrible => neutral\n",
      "Its awful that you cannot stop smoking => neutral\n",
      "Dogs scare me => neutral\n",
      "I am afraid I will get sick at work => neutral\n",
      "I run away when I see a dog => neutral\n",
      "When do you start your job? => neutral\n"
     ]
    }
   ],
   "source": [
    "# have classifier make a prediction\n",
    "\n",
    "some_chat_pred = svm_linear_clf.predict(some_chat_embedding_vectors)\n",
    "print('System predictions', some_chat_pred)\n",
    "print('Gold labels', some_chat_labels)\n",
    "for review, predicted_label in zip(some_chat, some_chat_pred):\n",
    "    \n",
    "    print('%s => %s' % (review, \n",
    "                        le.classes_[predicted_label]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger' 'disgust' 'fear' 'joy' 'neutral' 'sadness' 'surprise']\n",
      "Embedding SVM LINEAR ON EXTERNAL DATA -----------------------------\n",
      "Word embedding model used glove-twitter-25\n",
      "Word frequency threshold 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0  0.0000000 0.0000000 0.0000000         2\n",
      "           1  0.0000000 0.0000000 0.0000000         2\n",
      "           2  0.0000000 0.0000000 0.0000000         3\n",
      "           3  1.0000000 0.5000000 0.6666667         4\n",
      "           4  0.1538462 1.0000000 0.2666667         2\n",
      "           5  0.0000000 0.0000000 0.0000000         2\n",
      "           6  0.0000000 0.0000000 0.0000000         1\n",
      "\n",
      "    accuracy                      0.2500000        16\n",
      "   macro avg  0.1648352 0.2142857 0.1333333        16\n",
      "weighted avg  0.2692308 0.2500000 0.2000000        16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(some_chat_labels,some_chat_pred,digits = 7)\n",
    "print(le.classes_)\n",
    "print('Embedding SVM LINEAR ON EXTERNAL DATA -----------------------------')\n",
    "print('Word embedding model used', wordembeddings)\n",
    "print('Word frequency threshold', frequency_threshold)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare the results again with those from the BoW SVM of the previous notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "['anger' 'disgust' 'fear' 'joy' 'neutral' 'sadness' 'surprise']\n",
    "BOW SVM LINEAR ----------------------------------------------------------------\n",
    "Word freqeuncy threshold 5\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0  0.0000000 0.0000000 0.0000000         2\n",
    "           1  0.0000000 0.0000000 0.0000000         2\n",
    "           2  0.0000000 0.0000000 0.0000000         3\n",
    "           3  1.0000000 0.5000000 0.6666667         4\n",
    "           4  0.1538462 1.0000000 0.2666667         2\n",
    "           5  0.0000000 0.0000000 0.0000000         2\n",
    "           6  1.0000000 1.0000000 1.0000000         1\n",
    "\n",
    "    accuracy                      0.3125000        16\n",
    "   macro avg  0.3076923 0.3571429 0.2761905        16\n",
    "weighted avg  0.3317308 0.3125000 0.2625000        16\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the word embedding model works almost as good as the BoW classifier.\n",
    "\n",
    "This can be explained by the robustness of the dense vector representations. The MELD test data is from the same source as the MELD training data and the BoW SVM may be overfitting on all kinds of features. This is much less the case for the embedding based model, which is also more robust since it can handle input with words that do not even occur in the training data but belong to the same family."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Saving the classifier to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with the previous notebook, you can save the emotion classification model to disk and load the model some other time. Note that you need to load the same word2vec model as well to represent any text input with vector representations that are compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the classifier to disk\n",
    "filename_classifier = './models/svm_linear_clf_embeddings.sav'\n",
    "pickle.dump(svm_linear_clf, open(filename_classifier, 'wb'))\n",
    "\n",
    "filename_freq_keywords = './models/frequent_keywords.sav'\n",
    "pickle.dump(frequent_keywords, open(filename_freq_keywords, 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 16\n",
      "System predictions [3 6 4 4 4 4 4 4 3 4 4 4 4 4 4 4]\n",
      "Gold labels [3 3 4 5 0 3 0 6 3 5 1 1 2 2 2 4]\n",
      "That is sweet of you => joy\n",
      "You are so funny => surprise\n",
      "Are you a man or a woman? => neutral\n",
      "Chatbots make me sad and feel lonely. => neutral\n",
      "Your are stupid and boring. => neutral\n",
      "Two thumbs up => neutral\n",
      "I fell asleep halfway through this conversation => neutral\n",
      "Wow, I am really amazed. => neutral\n",
      "You are amazing. => joy\n",
      "I feel so low being in isolation => neutral\n",
      "People dumping waste are horrible => neutral\n",
      "Its awful that you cannot stop smoking => neutral\n",
      "Dogs scare me => neutral\n",
      "I am afraid I will get sick at work => neutral\n",
      "I run away when I see a dog => neutral\n",
      "When do you start your job? => neutral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/.local/lib/python3.7/site-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "# some time later...\n",
    " \n",
    "# load the classifier and the vectorizer from disk\n",
    "loaded_classifier = pickle.load(open(filename_classifier, 'rb'))\n",
    "loaded_frequent_keywords = pickle.load(open(filename_freq_keywords, 'rb'))\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "some_chat_tokens = tokenize_data(some_chat)\n",
    "some_chat_embedding_vectors = getAvgFeatureVecs(some_chat_tokens, loaded_frequent_keywords,stop_words, word_embedding_model, index2word_set, num_features)  \n",
    "some_chat_pred = loaded_classifier.predict(some_chat_embedding_vectors)\n",
    "\n",
    "print('System predictions', some_chat_pred)\n",
    "print('Gold labels', some_chat_labels)\n",
    "\n",
    "for review, predicted_label in zip(some_chat, some_chat_pred):\n",
    "    \n",
    "    print('%s => %s' % (review, \n",
    "                        le.classes_[predicted_label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
