{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3.4 Sentiment Analysis with machine-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The focus of this notebook is on performing sentiment analysis using the scikit-learn package. Material from [this notebook](http://www.pitt.edu/~naraehan/presentation/Movie+Reviews+sentiment+analysis+with+Scikit-Learn.html) was re-used and adapted.\n",
    "\n",
    "**At the end of this notebook, you will be able to**:\n",
    "* load the training data, i.e., the movie reviews\n",
    "* inspect the training data, i.e., the movie reviews\n",
    "* extracting features from the training data\n",
    "* training and evaluating the *NaiveBayesClassifier*\n",
    "* apply the classifier to fake movie reviews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training a machine learning system we need a number of packages, the most important ones are *sklearn* and *numpy* to manipulate data and call machine learning functions. Since we are dealing with texts, we also need some specific packages from *sklearn* to operate on texts to get words as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a dataset\n",
    "We are first going to load and inspect the **airlinetweets** dataset (which is included in the data folder that you downloaded from HLT course Github). This data set originates from:\n",
    "\n",
    "https://www.kaggle.com/crowdflower/twitter-airline-sentiment\n",
    "\n",
    "Kaggle is a platform where you can find many more data sets.\n",
    "\n",
    "We are going to use the method **load_files** as part of sklearn.\n",
    "Let's first inspect what the help message of the function **load_files** states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_files in module sklearn.datasets._base:\n",
      "\n",
      "load_files(container_path, *, description=None, categories=None, load_content=True, shuffle=True, encoding=None, decode_error='strict', random_state=0)\n",
      "    Load text files with categories as subfolder names.\n",
      "    \n",
      "    Individual samples are assumed to be files stored a two levels folder\n",
      "    structure such as the following:\n",
      "    \n",
      "        container_folder/\n",
      "            category_1_folder/\n",
      "                file_1.txt\n",
      "                file_2.txt\n",
      "                ...\n",
      "                file_42.txt\n",
      "            category_2_folder/\n",
      "                file_43.txt\n",
      "                file_44.txt\n",
      "                ...\n",
      "    \n",
      "    The folder names are used as supervised signal label names. The individual\n",
      "    file names are not important.\n",
      "    \n",
      "    This function does not try to extract features into a numpy array or scipy\n",
      "    sparse matrix. In addition, if load_content is false it does not try to\n",
      "    load the files in memory.\n",
      "    \n",
      "    To use text files in a scikit-learn classification or clustering algorithm,\n",
      "    you will need to use the :mod`~sklearn.feature_extraction.text` module to\n",
      "    build a feature extraction transformer that suits your problem.\n",
      "    \n",
      "    If you set load_content=True, you should also specify the encoding of the\n",
      "    text using the 'encoding' parameter. For many modern text files, 'utf-8'\n",
      "    will be the correct encoding. If you leave encoding equal to None, then the\n",
      "    content will be made of bytes instead of Unicode, and you will not be able\n",
      "    to use most functions in :mod:`~sklearn.feature_extraction.text`.\n",
      "    \n",
      "    Similar feature extractors should be built for other kind of unstructured\n",
      "    data input such as images, audio, video, ...\n",
      "    \n",
      "    Read more in the :ref:`User Guide <datasets>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    container_path : str\n",
      "        Path to the main folder holding one subfolder per category.\n",
      "    \n",
      "    description : str, default=None\n",
      "        A paragraph describing the characteristic of the dataset: its source,\n",
      "        reference, etc.\n",
      "    \n",
      "    categories : list of str, default=None\n",
      "        If None (default), load all the categories. If not None, list of\n",
      "        category names to load (other categories ignored).\n",
      "    \n",
      "    load_content : bool, default=True\n",
      "        Whether to load or not the content of the different files. If true a\n",
      "        'data' attribute containing the text information is present in the data\n",
      "        structure returned. If not, a filenames attribute gives the path to the\n",
      "        files.\n",
      "    \n",
      "    shuffle : bool, default=True\n",
      "        Whether or not to shuffle the data: might be important for models that\n",
      "        make the assumption that the samples are independent and identically\n",
      "        distributed (i.i.d.), such as stochastic gradient descent.\n",
      "    \n",
      "    encoding : str, default=None\n",
      "        If None, do not try to decode the content of the files (e.g. for images\n",
      "        or other non-text content). If not None, encoding to use to decode text\n",
      "        files to Unicode if load_content is True.\n",
      "    \n",
      "    decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
      "        Instruction on what to do if a byte sequence is given to analyze that\n",
      "        contains characters not of the given `encoding`. Passed as keyword\n",
      "        argument 'errors' to bytes.decode.\n",
      "    \n",
      "    random_state : int, RandomState instance or None, default=0\n",
      "        Determines random number generation for dataset shuffling. Pass an int\n",
      "        for reproducible output across multiple function calls.\n",
      "        See :term:`Glossary <random_state>`.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    data : :class:`~sklearn.utils.Bunch`\n",
      "        Dictionary-like object, with the following attributes.\n",
      "    \n",
      "        data : list of str\n",
      "            Only present when `load_content=True`.\n",
      "            The raw text data to learn.\n",
      "        target : ndarray\n",
      "            The target labels (integer index).\n",
      "        target_names : list\n",
      "            The names of target classes.\n",
      "        DESCR : str\n",
      "            The full description of the dataset.\n",
      "        filenames: ndarray\n",
      "            The filenames holding the dataset.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(load_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so the function requires the following structure of the data on your disk in order for it to work:\n",
    "\n",
    "* container_folder/\n",
    "    * category_1_folder/ (e.g., 'positive')\n",
    "        * file_1.txt\n",
    "        * file_2.txt\n",
    "        * ...\n",
    "        file_42.txt\n",
    "    * category_2_folder/ (e.g., 'negative')\n",
    "        * file_43.txt\n",
    "        * file_44.txt\n",
    "        * ...\n",
    "        \n",
    "The names of the subfolders are treated as the labels for the data and you are supposed to divided the text files over the subfolders accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check whether our **airlinetweets** corpus has this structure. In the data folder, you will see a folder \"airlinetweets\" with 3 subfolders: \"negative\", \"neutral\", and \"positive\". These subfolder names will represent our sentiment labels. In each folder, you find a long list of text files, each containing a tweet. This is our training and test data per category.\n",
    "\n",
    "We will use the **pathlib** package to get the path of this notebook and extend it with \"data/airlinetweets\", assuming that you stored the data in this location. If this is not the case, you need to adapt the path accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: /Users/piek/Desktop/t-ONDERWIJS/2021-2022/t-MA-HLT-introduction-2021/ma-hlt-labs/lab3.machine_learning/data/airlinetweets\n",
      "this will print True if the folder exists: True\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "cwd = pathlib.Path.cwd()\n",
    "airline_tweets_folder = cwd.joinpath('data/airlinetweets')\n",
    "print('path:', airline_tweets_folder)\n",
    "print('this will print True if the folder exists:', \n",
    "      airline_tweets_folder.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/piek/Desktop/t-ONDERWIJS/2021-2022/t-MA-HLT-introduction-2021/ma-hlt-labs/lab3.machine_learning/data/airlinetweets'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(airline_tweets_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect on your local machine whether the data has the required structure. Take your time to open files and inspect them. It is always good to know what forms your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, it is! Let's now load it using the **load_files** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading all files as training data.\n",
    "airline_tweets_train = load_files(str(airline_tweets_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The load_files function will read each file separately and store it as a data item or instance. The data is internally stored as a specfic type of object called *Bunch*, which consists of a description 'DESCR', the names of the loaded files, the target data and the category labels (target_names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils.Bunch'>\n",
      "['DESCR', 'data', 'filenames', 'target', 'target_names']\n",
      "<class 'numpy.ndarray'>\n",
      "Target names ['negative', 'neutral', 'positive']\n"
     ]
    }
   ],
   "source": [
    "print(type(airline_tweets_train))\n",
    "print(dir(airline_tweets_train))\n",
    "print(type(airline_tweets_train.target))\n",
    "print('Target names', airline_tweets_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many files do we have? Well, that would be the same as the number of data items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data items created from the laoded files 4755\n"
     ]
    }
   ],
   "source": [
    "print('Number of data items created from the laoded files', len(airline_tweets_train.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not like with the data labels, you could change the names of the subdirectories. If you do not agree with the distinctions, you could add other folders with other category names and move files to these folders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that *target* is a **numpy.ndarray** that represents the index to the category values for all the data that we have loaded, in this case all the tweet files: 4,755 in total. The category values are represented in a list of target_names so the value '0' represents 'negative', '1' represents 'neutral' and '2' represents 'positive'. Let's have a look. Because it is a *numpy* object, we need to convert it to a list to print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4755\n",
      "[1, 1, 2, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 2, 1, 0, 0, 2, 0, 2, 2, 2, 1, 2, 1, 0, 2, 2, 0, 0, 1, 0, 2, 0, 0, 1, 1, 1, 1, 1, 2, 0, 0, 0, 1, 0, 0, 1, 2, 0, 2, 0, 2, 0, 1, 0, 2, 2, 2, 0, 1, 2, 2, 2, 2, 0, 1, 1, 2, 1, 0, 0, 1, 1, 1, 0, 0, 2, 0, 1, 0, 1, 2, 1, 0, 2, 2, 0, 2, 0, 0, 2, 2, 0, 2, 2, 0, 1, 0, 1, 0, 0, 0, 0, 1, 2, 1, 2, 2, 2, 1, 0, 2, 2, 0, 0, 2, 0, 0, 1, 1, 1, 2, 1, 0, 0, 2, 0, 1, 1, 0, 1, 2, 1, 0, 2, 1, 0, 2, 0, 1, 0, 2, 1, 2, 0, 1, 1, 2, 0, 2, 2, 0, 2, 2, 0, 1, 1, 0, 2, 1, 1, 1, 2, 1, 2, 1, 0, 0, 1, 2, 1, 2, 2, 0, 2, 1, 0, 2, 0, 2, 0, 1, 2, 2, 0, 2, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 1, 2, 1, 2, 2, 1, 0, 2, 0, 2, 0, 2, 0, 0, 0, 1, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1, 2, 2, 1, 0, 1, 0, 1, 1, 2, 0, 2, 0, 2, 1, 0, 0, 2, 0, 0, 0, 2, 1, 0, 2, 2, 0, 1, 2, 0, 0, 2, 1, 2, 1, 0, 1, 2, 2, 2, 2, 2, 2, 1, 1, 0, 0, 0, 2, 2, 2, 0, 2, 1, 2, 1, 1, 2, 0, 1, 1, 2, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 2, 1, 1, 0, 2, 1, 2, 0, 2, 0, 0, 0, 0, 2, 2, 1, 0, 2, 0, 0, 1, 1, 2, 0, 2, 0, 2, 0, 0, 2, 2, 0, 1, 0, 1, 0, 0, 2, 0, 2, 2, 1, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 2, 0, 1, 2, 0, 0, 1, 1, 1, 0, 0, 2, 2, 2, 1, 2, 1, 0, 0, 2, 1, 0, 2, 2, 0, 1, 2, 1, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 1, 0, 2, 0, 2, 2, 0, 0, 2, 0, 1, 2, 0, 0, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 2, 1, 0, 0, 1, 2, 0, 0, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 0, 1, 1, 0, 2, 1, 1, 2, 2, 0, 1, 2, 2, 2, 1, 0, 1, 2, 0, 2, 0, 0, 0, 0, 0, 1, 1, 2, 1, 0, 0, 2, 1, 0, 1, 0, 0, 0, 2, 1, 2, 2, 2, 0, 1, 0, 0, 1, 1, 2, 1, 1, 2, 0, 1, 1, 1, 0, 1, 2, 1, 2, 1, 0, 0, 2, 0, 1, 0, 1, 2, 1, 1, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 0, 0, 1, 1, 0, 2, 1, 0, 2, 0, 0, 0, 1, 2, 2, 1, 2, 0, 2, 0, 1, 0, 2, 1, 2, 2, 1, 2, 1, 0, 1, 0, 1, 0, 0, 2, 2, 0, 2, 0, 1, 0, 2, 1, 0, 0, 2, 2, 1, 1, 1, 0, 1, 0, 0, 1, 2, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 0, 2, 2, 1, 0, 0, 2, 0, 1, 0, 0, 1, 2, 0, 1, 2, 0, 1, 2, 1, 0, 1, 2, 0, 2, 1, 2, 0, 0, 2, 1, 0, 2, 2, 2, 0, 2, 1, 0, 1, 1, 0, 1, 1, 0, 2, 0, 0, 0, 2, 1, 2, 2, 1, 2, 0, 0, 1, 2, 2, 2, 0, 1, 0, 0, 1, 0, 0, 1, 1, 2, 1, 1, 0, 2, 2, 0, 0, 0, 2, 0, 2, 1, 0, 1, 1, 2, 2, 0, 0, 1, 1, 1, 2, 1, 2, 2, 0, 1, 0, 2, 2, 0, 0, 0, 2, 1, 2, 0, 2, 0, 1, 2, 1, 1, 1, 2, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 2, 1, 2, 0, 1, 1, 2, 1, 1, 1, 0, 2, 0, 1, 0, 1, 1, 2, 1, 2, 2, 1, 2, 0, 0, 0, 1, 1, 1, 1, 2, 1, 2, 0, 1, 2, 0, 2, 2, 0, 1, 1, 0, 1, 1, 1, 2, 1, 0, 2, 2, 0, 0, 1, 1, 2, 1, 2, 2, 0, 1, 2, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 2, 2, 1, 0, 2, 2, 1, 1, 1, 0, 1, 0, 2, 2, 0, 2, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 2, 1, 0, 2, 1, 1, 0, 1, 2, 0, 2, 1, 0, 1, 0, 1, 0, 0, 2, 2, 1, 1, 1, 0, 1, 2, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 2, 1, 0, 0, 2, 2, 0, 0, 0, 2, 0, 1, 2, 0, 2, 0, 2, 0, 1, 2, 0, 2, 0, 1, 0, 1, 2, 2, 0, 2, 0, 2, 1, 1, 1, 2, 2, 2, 0, 1, 1, 1, 1, 0, 1, 2, 0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 2, 2, 2, 2, 1, 0, 0, 0, 2, 1, 1, 2, 2, 2, 0, 0, 1, 1, 2, 1, 0, 2, 1, 2, 2, 0, 0, 0, 0, 0, 2, 1, 2, 0, 2, 0, 2, 1, 1, 0, 0, 2, 2, 0, 1, 0, 1, 2, 0, 2, 0, 1, 1, 0, 1, 1, 0, 2, 0, 0, 0, 1, 1, 1, 2, 0, 0, 2, 0, 0, 2, 2, 2, 2, 0, 1, 2, 2, 0, 1, 1, 1, 0, 0, 2, 0, 2, 0, 0, 0, 2, 1, 1, 0, 2, 1, 2, 2, 0, 2, 1, 2, 1, 1, 1, 2, 0, 1, 2, 1, 2, 0, 2, 0, 0, 2, 1, 1, 0, 0, 2, 0, 2, 0, 1, 2, 0, 0, 1, 0, 1, 1, 1, 2, 1, 0, 1, 0, 2, 1, 1, 0, 2, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 2, 2, 0, 0, 2, 0, 0, 0, 1, 1, 1, 0, 2, 2, 0, 0, 0, 0, 0, 2, 0, 1, 2, 0, 0, 2, 0, 2, 0, 0, 0, 2, 2, 0, 0, 1, 0, 0, 2, 1, 1, 2, 1, 0, 1, 2, 2, 0, 1, 0, 2, 1, 1, 0, 2, 1, 1, 1, 2, 2, 2, 1, 0, 1, 0, 2, 1, 2, 0, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 2, 0, 0, 0, 2, 1, 2, 2, 0, 0, 2, 0, 2, 1, 0, 2, 1, 1, 2, 1, 1, 2, 2, 0, 0, 2, 1, 2, 1, 2, 2, 1, 0, 1, 1, 2, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 2, 2, 1, 2, 2, 2, 2, 1, 1, 0, 1, 2, 1, 0, 2, 0, 0, 1, 2, 1, 0, 0, 2, 0, 2, 0, 2, 0, 0, 1, 2, 1, 1, 0, 0, 1, 2, 2, 2, 0, 1, 2, 2, 2, 0, 2, 1, 2, 2, 0, 0, 1, 2, 0, 1, 1, 1, 0, 2, 1, 1, 2, 2, 0, 0, 0, 0, 1, 1, 2, 2, 0, 1, 0, 0, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 0, 1, 1, 2, 1, 2, 2, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 2, 0, 1, 2, 1, 0, 0, 2, 2, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 2, 0, 2, 1, 0, 0, 2, 2, 1, 1, 2, 0, 0, 2, 1, 0, 0, 1, 2, 0, 2, 1, 0, 0, 2, 0, 2, 0, 0, 1, 2, 0, 0, 0, 2, 0, 0, 1, 1, 2, 1, 2, 0, 2, 0, 2, 1, 0, 1, 2, 1, 2, 2, 1, 0, 0, 0, 2, 2, 0, 0, 1, 2, 1, 0, 1, 0, 0, 0, 0, 1, 1, 2, 2, 0, 1, 1, 0, 0, 1, 2, 1, 2, 1, 2, 0, 2, 1, 2, 2, 2, 2, 1, 2, 2, 0, 1, 2, 0, 1, 1, 0, 1, 2, 2, 2, 0, 1, 1, 0, 1, 1, 0, 0, 2, 0, 0, 1, 2, 2, 2, 2, 1, 1, 0, 2, 2, 0, 2, 0, 2, 1, 0, 2, 2, 0, 1, 0, 0, 2, 1, 1, 2, 2, 1, 0, 2, 2, 1, 0, 1, 1, 0, 1, 2, 2, 1, 0, 0, 0, 0, 0, 2, 2, 2, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0, 2, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 0, 2, 2, 2, 1, 2, 0, 2, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 1, 2, 1, 1, 0, 2, 2, 1, 0, 0, 2, 1, 1, 2, 2, 1, 2, 0, 2, 2, 0, 1, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 1, 2, 2, 1, 0, 0, 2, 0, 1, 2, 2, 2, 1, 1, 1, 1, 0, 2, 1, 0, 1, 1, 1, 0, 1, 0, 2, 1, 2, 1, 0, 1, 2, 0, 1, 0, 2, 2, 2, 2, 1, 1, 0, 0, 2, 2, 1, 1, 2, 2, 0, 2, 2, 2, 0, 1, 2, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 0, 0, 1, 0, 1, 2, 0, 2, 2, 1, 1, 2, 0, 1, 1, 2, 0, 1, 0, 2, 0, 1, 0, 0, 0, 1, 0, 2, 0, 1, 2, 0, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 1, 2, 1, 0, 1, 0, 1, 1, 2, 1, 2, 2, 2, 0, 0, 0, 0, 2, 2, 1, 0, 2, 0, 1, 2, 2, 0, 1, 1, 1, 0, 2, 0, 0, 0, 2, 2, 2, 1, 0, 2, 1, 0, 0, 1, 0, 0, 2, 1, 2, 2, 2, 0, 0, 0, 1, 1, 0, 0, 2, 0, 2, 1, 0, 1, 1, 0, 0, 2, 1, 0, 1, 0, 1, 2, 2, 2, 0, 2, 2, 0, 0, 1, 1, 1, 1, 2, 1, 2, 0, 1, 2, 0, 0, 2, 2, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 2, 0, 0, 2, 2, 1, 2, 1, 1, 0, 0, 2, 0, 0, 2, 1, 2, 2, 2, 1, 2, 0, 0, 1, 2, 2, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 1, 0, 1, 1, 2, 2, 1, 0, 0, 2, 1, 2, 2, 0, 1, 0, 2, 0, 2, 2, 2, 1, 0, 0, 1, 2, 0, 0, 0, 0, 2, 1, 2, 1, 0, 1, 2, 1, 0, 0, 0, 1, 0, 2, 0, 2, 0, 1, 2, 1, 2, 1, 0, 2, 0, 0, 0, 1, 0, 0, 1, 2, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 2, 0, 0, 2, 2, 1, 0, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 0, 2, 2, 0, 1, 1, 2, 2, 1, 2, 0, 2, 0, 2, 2, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 2, 0, 2, 0, 0, 2, 1, 0, 2, 0, 0, 1, 2, 0, 1, 2, 0, 2, 2, 1, 0, 2, 2, 1, 0, 2, 0, 1, 1, 1, 1, 1, 0, 1, 0, 2, 0, 1, 0, 1, 1, 2, 1, 0, 0, 0, 0, 2, 1, 2, 1, 2, 2, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 2, 0, 1, 0, 2, 0, 0, 2, 0, 1, 1, 2, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 2, 0, 0, 2, 2, 2, 2, 0, 1, 0, 2, 0, 2, 0, 2, 1, 1, 1, 1, 1, 1, 2, 1, 0, 0, 0, 1, 2, 1, 2, 1, 1, 0, 0, 0, 1, 2, 1, 0, 1, 0, 2, 1, 2, 0, 1, 1, 2, 0, 1, 1, 2, 2, 0, 2, 1, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0, 0, 1, 1, 2, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 2, 1, 2, 1, 2, 0, 1, 1, 1, 1, 0, 2, 0, 2, 1, 1, 1, 1, 2, 0, 0, 1, 0, 1, 0, 2, 0, 0, 2, 0, 2, 2, 1, 0, 1, 1, 0, 0, 0, 0, 1, 2, 1, 2, 2, 0, 2, 1, 2, 2, 1, 0, 1, 2, 0, 2, 0, 1, 0, 1, 1, 0, 2, 0, 1, 2, 1, 2, 2, 1, 0, 1, 2, 1, 2, 0, 0, 0, 2, 2, 2, 0, 0, 0, 2, 1, 0, 1, 1, 2, 1, 0, 2, 0, 0, 1, 2, 0, 0, 1, 1, 1, 0, 1, 2, 0, 0, 2, 1, 0, 0, 1, 1, 2, 2, 1, 0, 0, 0, 0, 2, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 2, 0, 1, 1, 0, 2, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 2, 2, 0, 1, 0, 0, 1, 0, 1, 0, 2, 2, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 2, 0, 2, 2, 2, 2, 1, 1, 1, 1, 1, 0, 1, 0, 2, 0, 0, 0, 1, 1, 2, 2, 0, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0, 2, 1, 2, 2, 0, 2, 0, 2, 0, 0, 2, 1, 2, 1, 2, 1, 0, 2, 2, 2, 1, 2, 0, 1, 2, 1, 0, 0, 2, 2, 0, 1, 0, 0, 2, 2, 0, 0, 1, 1, 2, 2, 0, 2, 1, 1, 2, 2, 1, 2, 0, 1, 2, 2, 1, 0, 0, 0, 0, 2, 0, 1, 1, 1, 0, 2, 0, 2, 1, 2, 0, 1, 0, 0, 1, 1, 1, 2, 0, 2, 0, 1, 2, 1, 2, 0, 0, 1, 0, 2, 0, 0, 1, 2, 2, 2, 0, 2, 0, 0, 1, 0, 1, 0, 2, 2, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 2, 0, 0, 0, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 0, 2, 2, 0, 2, 0, 0, 0, 0, 0, 1, 2, 0, 2, 2, 0, 0, 2, 1, 2, 0, 1, 1, 1, 0, 0, 1, 2, 0, 0, 1, 0, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 1, 1, 2, 2, 2, 0, 2, 2, 2, 1, 0, 2, 0, 0, 1, 1, 2, 1, 0, 2, 1, 2, 2, 1, 0, 1, 0, 0, 0, 2, 1, 2, 0, 2, 2, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 2, 2, 2, 1, 2, 1, 0, 2, 2, 1, 0, 0, 0, 1, 2, 0, 1, 2, 2, 0, 0, 1, 2, 0, 2, 0, 0, 1, 0, 0, 0, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1, 2, 0, 2, 1, 2, 0, 2, 0, 0, 2, 2, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 2, 2, 0, 1, 2, 2, 0, 1, 1, 2, 0, 2, 2, 0, 0, 0, 1, 2, 1, 0, 2, 0, 1, 0, 0, 0, 1, 1, 1, 1, 2, 2, 0, 2, 0, 0, 2, 0, 1, 2, 1, 2, 0, 2, 0, 2, 2, 0, 1, 1, 0, 2, 1, 0, 0, 2, 2, 1, 1, 2, 1, 0, 2, 2, 1, 2, 2, 1, 0, 1, 2, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 2, 1, 1, 0, 2, 0, 1, 2, 0, 0, 1, 0, 1, 2, 1, 2, 1, 1, 2, 2, 2, 0, 1, 2, 2, 1, 2, 0, 2, 1, 2, 0, 0, 0, 2, 2, 2, 1, 1, 0, 0, 2, 0, 1, 0, 0, 2, 1, 1, 1, 2, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 2, 2, 0, 0, 2, 0, 1, 1, 2, 0, 0, 1, 2, 2, 1, 0, 0, 1, 0, 1, 0, 0, 1, 2, 0, 0, 1, 2, 1, 2, 0, 2, 2, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 2, 1, 2, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 1, 0, 1, 1, 1, 1, 2, 2, 2, 1, 1, 0, 1, 2, 1, 1, 1, 2, 2, 2, 2, 1, 2, 0, 2, 0, 1, 2, 1, 2, 1, 0, 1, 1, 1, 2, 2, 0, 1, 2, 0, 0, 0, 0, 2, 2, 0, 2, 1, 1, 2, 1, 2, 0, 0, 0, 0, 1, 0, 1, 1, 2, 2, 1, 2, 0, 1, 0, 1, 2, 0, 2, 2, 0, 1, 0, 0, 1, 1, 2, 0, 0, 2, 0, 1, 2, 0, 1, 2, 1, 0, 2, 2, 0, 1, 2, 1, 1, 2, 1, 0, 1, 1, 0, 2, 2, 2, 2, 2, 1, 2, 2, 0, 1, 1, 0, 0, 0, 2, 2, 2, 2, 1, 1, 0, 1, 1, 0, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 0, 1, 2, 1, 0, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 0, 2, 1, 0, 0, 2, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 0, 2, 1, 0, 2, 2, 0, 0, 2, 2, 1, 2, 2, 0, 1, 1, 2, 2, 1, 1, 1, 2, 2, 0, 1, 2, 1, 0, 2, 1, 2, 0, 2, 2, 2, 2, 1, 2, 0, 1, 2, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 2, 1, 0, 2, 0, 2, 1, 0, 0, 0, 2, 1, 1, 1, 2, 0, 2, 1, 2, 1, 0, 0, 2, 1, 0, 1, 2, 1, 0, 2, 2, 0, 1, 0, 0, 1, 1, 1, 2, 0, 1, 0, 0, 2, 2, 0, 0, 1, 2, 1, 2, 0, 0, 1, 1, 1, 1, 0, 0, 2, 0, 0, 0, 1, 0, 0, 1, 1, 0, 2, 2, 2, 1, 0, 2, 1, 2, 0, 2, 0, 1, 1, 2, 2, 1, 1, 2, 0, 1, 0, 2, 2, 0, 2, 2, 0, 1, 0, 1, 1, 0, 2, 0, 0, 2, 1, 0, 1, 2, 2, 0, 0, 1, 2, 1, 0, 1, 0, 1, 0, 0, 1, 2, 0, 1, 2, 2, 1, 0, 2, 2, 2, 0, 1, 1, 0, 2, 1, 0, 1, 0, 0, 0, 0, 2, 1, 2, 2, 0, 2, 0, 2, 1, 0, 2, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 0, 2, 0, 0, 2, 0, 2, 2, 1, 1, 1, 0, 1, 0, 1, 2, 0, 1, 0, 0, 0, 1, 0, 0, 2, 1, 1, 2, 2, 0, 2, 0, 0, 0, 1, 1, 2, 1, 1, 1, 2, 2, 0, 0, 2, 0, 2, 0, 2, 2, 1, 0, 1, 2, 0, 0, 2, 2, 0, 1, 1, 1, 2, 2, 1, 2, 0, 1, 0, 2, 2, 1, 0, 1, 1, 1, 2, 1, 1, 0, 1, 1, 2, 2, 0, 0, 0, 0, 1, 2, 1, 2, 1, 1, 1, 0, 0, 1, 0, 2, 2, 1, 2, 2, 0, 0, 0, 0, 2, 0, 2, 2, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 2, 1, 1, 2, 0, 1, 2, 0, 0, 0, 2, 2, 2, 2, 2, 2, 1, 0, 2, 2, 1, 0, 0, 0, 1, 2, 0, 0, 2, 1, 0, 0, 2, 2, 2, 0, 1, 1, 0, 1, 0, 2, 0, 0, 1, 0, 1, 1, 1, 1, 2, 0, 2, 2, 2, 2, 0, 1, 0, 0, 2, 1, 1, 2, 2, 0, 1, 1, 0, 1, 2, 2, 2, 0, 2, 0, 1, 1, 1, 1, 2, 2, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 0, 1, 2, 2, 0, 0, 0, 1, 0, 1, 1, 2, 0, 1, 1, 0, 0, 1, 1, 2, 0, 0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 0, 1, 2, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 2, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 1, 1, 2, 1, 2, 0, 1, 0, 0, 1, 0, 2, 0, 2, 0, 1, 2, 2, 0, 0, 1, 1, 1, 2, 2, 1, 2, 1, 0, 2, 0, 0, 0, 2, 2, 2, 0, 0, 1, 2, 1, 0, 0, 1, 2, 1, 2, 2, 0, 1, 0, 0, 1, 1, 2, 2, 0, 1, 0, 2, 0, 0, 0, 1, 0, 2, 1, 2, 0, 0, 1, 0, 0, 0, 2, 2, 0, 2, 0, 0, 0, 2, 2, 0, 0, 1, 2, 0, 1, 0, 0, 1, 2, 0, 2, 0, 2, 1, 0, 1, 2, 0, 0, 1, 0, 0, 1, 2, 2, 1, 0, 2, 1, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 1, 0, 2, 1, 1, 1, 2, 1, 2, 0, 2, 0, 2, 2, 1, 1, 1, 0, 1, 0, 1, 0, 2, 0, 2, 2, 0, 1, 2, 2, 2, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 0, 0, 0, 1, 1, 2, 2, 2, 2, 1, 2, 2, 0, 0, 1, 0, 2, 1, 0, 2, 2, 1, 0, 0, 0, 2, 1, 1, 1, 1, 1, 0, 2, 0, 1, 1, 0, 0, 1, 2, 2, 2, 1, 2, 2, 0, 2, 2, 1, 1, 1, 1, 0, 1, 0, 1, 2, 2, 2, 2, 1, 0, 0, 2, 2, 0, 0, 2, 0, 0, 1, 0, 1, 1, 1, 2, 2, 2, 0, 0, 1, 0, 2, 1, 1, 2, 1, 0, 2, 0, 2, 2, 0, 2, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 2, 2, 1, 1, 2, 2, 0, 0, 1, 2, 2, 2, 2, 0, 0, 2, 2, 1, 0, 0, 0, 0, 0, 0, 2, 2, 1, 1, 0, 0, 1, 1, 1, 1, 2, 0, 1, 1, 0, 0, 0, 1, 2, 2, 2, 0, 0, 1, 0, 2, 2, 0, 1, 0, 0, 1, 0, 2, 2, 1, 1, 2, 1, 0, 2, 0, 0, 2, 2, 2, 1, 0, 0, 0, 2, 1, 1, 0, 1, 2, 0, 1, 0, 0, 0, 2, 2, 2, 0, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 2, 2, 0, 1, 0, 0, 0, 0, 0, 2, 1, 1, 0, 1, 0, 1, 1, 0, 0, 2, 0, 2, 2, 1, 0, 2, 2, 0, 1, 1, 2, 1, 1, 2, 0, 0, 2, 1, 0, 2, 1, 1, 1, 1, 0, 0, 2, 2, 0, 1, 2, 0, 0, 2, 2, 0, 2, 0, 2, 0, 0, 1, 1, 0, 0, 2, 2, 2, 2, 0, 1, 1, 2, 0, 2, 2, 0, 2, 1, 2, 1, 2, 0, 2, 0, 2, 2, 2, 1, 2, 1, 2, 2, 2, 0, 2, 1, 1, 0, 0, 1, 0, 2, 2, 0, 1, 2, 2, 1, 2, 2, 1, 1, 2, 0, 2, 2, 1, 2, 2, 1, 0, 0, 1, 0, 2, 1, 0, 0, 1, 1, 0, 2, 2, 0, 2, 2, 1, 2, 2, 1, 1, 2, 1, 0, 2, 2, 2, 2, 1, 0, 1, 2, 0, 2, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 2, 1, 0, 2, 2, 2, 1, 1, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 0, 1, 2, 1, 1, 2, 0, 0, 1, 2, 1, 0, 2, 0, 0, 1, 1, 2, 2, 2, 0, 0, 2, 2, 1, 2, 0, 2, 1, 1, 0, 0, 2, 1, 0, 0, 2, 1, 1, 0, 1, 1, 2, 2, 0, 2, 2, 2, 1, 2, 0, 2, 0, 1, 1, 1, 0, 0, 0, 2, 2, 2, 0, 2, 0, 1, 0, 2, 2, 1, 1, 1, 1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 1, 1, 2, 0, 2, 0, 2, 1, 1, 2, 0, 2, 0, 2, 1, 1, 2, 0, 0, 0, 2, 0, 0, 2, 0, 0, 1, 2, 2, 0, 2, 2, 2, 0, 2, 2, 1, 1, 1, 1, 0, 0, 2, 1, 0, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 0, 2, 2, 2, 2, 0, 1, 2, 1, 0, 1, 0, 1, 2, 0, 2, 0, 2, 0, 0, 0, 0, 1, 0, 2, 1, 0, 2, 0, 1, 0, 0, 2, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0, 1, 0, 2, 1, 2, 2, 1, 2, 0, 0, 2, 1, 0, 0, 1, 2, 2, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 2, 2, 1, 2, 2, 0, 1, 1, 2, 0, 0, 1, 2, 0, 0, 0, 1, 2, 0, 0, 0, 2, 1, 0, 2, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 2, 1, 0, 2, 2, 1, 2, 1, 2, 1, 0, 0, 2, 1, 2, 1, 0, 2, 2, 1, 1, 0, 2, 2, 0, 0, 0, 2, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 0, 0, 2, 0, 1, 0, 1, 0, 0, 0, 2, 2, 0, 0, 2, 1, 1, 2, 1, 1, 2, 1, 0, 2, 1, 2, 0, 1, 1, 1, 2, 0, 2, 2, 1, 0, 2, 0, 0, 1, 0, 2, 0, 1, 1, 2, 0, 2, 2, 0, 1, 2, 1, 2, 0, 1, 0, 0, 0, 1, 2, 2, 0, 2, 0, 1, 2, 0, 1, 2, 0, 2, 1, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 2, 0, 0, 0, 0, 0, 2, 0, 2, 0, 1, 2, 2, 0, 0, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 1, 0, 1, 2, 0, 2, 1, 1, 0, 2, 0, 2, 1, 0, 2, 0, 1, 1, 0, 0, 0, 0, 1, 2, 1, 1, 1, 0, 0, 2, 2, 2, 1, 1, 0, 2, 2, 1, 2, 0, 1, 1, 0, 2, 0, 1, 0, 2, 2, 0, 2, 0, 1, 1, 2, 2, 0, 1, 1, 2, 0, 0, 1, 1, 2, 2, 2, 0, 2, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 2, 1, 1, 2, 1, 0, 0, 2, 1, 1, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 2, 1, 2, 1, 1, 0, 2, 2, 2, 1, 1, 0, 1, 1, 2, 0, 0, 1, 0, 1, 2, 0, 2, 0, 0, 0, 2, 1, 1, 2, 0, 0, 2, 0, 0, 2, 1, 2, 1, 1, 0, 1, 1, 2, 0, 1, 2, 1, 2, 2, 2, 1, 0, 2, 2, 0, 2, 2, 1, 2, 1, 0, 0, 1, 2, 1, 0, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 2, 2, 0, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(len(airline_tweets_train.target.tolist()))\n",
    "print(airline_tweets_train.target.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the numbers are not the vector representations of the words!!\n",
    "\n",
    "We see a sequence of 0, 1, 2 values which represents the sequence of categories for the sequence of loaded texts. The first two are neutral (1) and the third is positive (2, the fifth negative (0).\n",
    "\n",
    "Let's is look at the actual text instances. What is the first text, which is supposed to be neutral:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'@AmericanAir Why is your cover photo of TWA? Just wondering.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's inspect the first data element\n",
    "airline_tweets_train.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/piek/Desktop/t-ONDERWIJS/2021-2022/t-MA-HLT-introduction-2021/ma-hlt-labs/lab3.machine_learning/data/airlinetweets/neutral/AL_570069345818161152.txt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# which comes from the first file is in \"neutral\" folder\n",
    "airline_tweets_train.filenames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first file is a neutral review and is mapped to index 1 in target_names\n",
    "airline_tweets_train.target[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find out what the index means by inserting it into **target_names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_tweets_train.target_names[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many do we have for each category? We can use the **Counter** package in Python to get statistics from the target structure by counting each category value and displaying the category using the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.Counter'>\n",
      "1 1515\n",
      "neutral 1515\n",
      "2 1490\n",
      "positive 1490\n",
      "0 1750\n",
      "negative 1750\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "freqs = Counter(airline_tweets_train.target)\n",
    "print(type(freqs))\n",
    "for category, frequency in freqs.items():\n",
    "    print(category, frequency)\n",
    "    print(airline_tweets_train.target_names[category], frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the data are equally distributed over the three categories, which is good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading another data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the **load_files** function, we can easily load any text collection per category by putting them into separate subfolders.  Let's try another one. Remember from the first Lab that some of the NLTK data is structured precisely in the same way as the airlinetweets data, e.g. nltk_data/corpora/movie_reviews.\n",
    "\n",
    "Likewise, we can load this data in the same way using the load_files function. First adapt the path below to point to the location on your local laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: /Users/piek/nltk_data/corpora/movie_reviews\n",
      "this will print True if the folder exists: True\n"
     ]
    }
   ],
   "source": [
    "movie_reviews_folder = cwd.joinpath('/Users/piek/nltk_data/corpora/movie_reviews')\n",
    "print('path:', movie_reviews_folder)\n",
    "print('this will print True if the folder exists:', \n",
    "      movie_reviews_folder.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews_train = load_files(str(movie_reviews_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils.Bunch'>\n",
      "['DESCR', 'data', 'filenames', 'target', 'target_names']\n",
      "Number of data items 2000\n",
      "Target names ['neg', 'pos']\n",
      "First data record b\"arnold schwarzenegger has been an icon for action enthusiasts , since the late 80's , but lately his films have been very sloppy and the one-liners are getting worse . \\nit's hard seeing arnold as mr . freeze in batman and robin , especially when he says tons of ice jokes , but hey he got 15 million , what's it matter to him ? \\nonce again arnold has signed to do another expensive blockbuster , that can't compare with the likes of the terminator series , true lies and even eraser . \\nin this so called dark thriller , the devil ( gabriel byrne ) has come upon earth , to impregnate a woman ( robin tunney ) which happens every 1000 years , and basically destroy the world , but apparently god has chosen one man , and that one man is jericho cane ( arnold himself ) . \\nwith the help of a trusty sidekick ( kevin pollack ) , they will stop at nothing to let the devil take over the world ! \\nparts of this are actually so absurd , that they would fit right in with dogma . \\nyes , the film is that weak , but it's better than the other blockbuster right now ( sleepy hollow ) , but it makes the world is not enough look like a 4 star film . \\nanyway , this definitely doesn't seem like an arnold movie . \\nit just wasn't the type of film you can see him doing . \\nsure he gave us a few chuckles with his well known one-liners , but he seemed confused as to where his character and the film was going . \\nit's understandable , especially when the ending had to be changed according to some sources . \\naside form that , he still walked through it , much like he has in the past few films . \\ni'm sorry to say this arnold but maybe these are the end of your action days . \\nspeaking of action , where was it in this film ? \\nthere was hardly any explosions or fights . \\nthe devil made a few places explode , but arnold wasn't kicking some devil butt . \\nthe ending was changed to make it more spiritual , which undoubtedly ruined the film . \\ni was at least hoping for a cool ending if nothing else occurred , but once again i was let down . \\ni also don't know why the film took so long and cost so much . \\nthere was really no super affects at all , unless you consider an invisible devil , who was in it for 5 minutes tops , worth the overpriced budget . \\nthe budget should have gone into a better script , where at least audiences could be somewhat entertained instead of facing boredom . \\nit's pitiful to see how scripts like these get bought and made into a movie . \\ndo they even read these things anymore ? \\nit sure doesn't seem like it . \\nthankfully gabriel's performance gave some light to this poor film . \\nwhen he walks down the street searching for robin tunney , you can't help but feel that he looked like a devil . \\nthe guy is creepy looking anyway ! \\nwhen it's all over , you're just glad it's the end of the movie . \\ndon't bother to see this , if you're expecting a solid action flick , because it's neither solid nor does it have action . \\nit's just another movie that we are suckered in to seeing , due to a strategic marketing campaign . \\nsave your money and see the world is not enough for an entertaining experience . \\n\"\n"
     ]
    }
   ],
   "source": [
    "print(type(movie_reviews_train))\n",
    "print(dir(movie_reviews_train))\n",
    "print('Number of data items', len(movie_reviews_train.data))\n",
    "print('Target names', movie_reviews_train.target_names)\n",
    "print('First data record', movie_reviews_train.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically, we get a similar data structure with text data and the labels \"neg\" and \"pos\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features from training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have loaded two data sets as instances of the object <class 'sklearn.utils.Bunch'> in our notebook. Next, we are going to use the CountVectorizer function to process the text data and to create a **Bag-of-Words** presentation as we have done before for the toy data. We define a separate instance of CountVectorizer for each data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize airline object, and then turn airline tweets train data into a vector \n",
    "\n",
    "airline_vec = CountVectorizer(min_df=10, # If a token appears fewer times than this, across all documents, it will be ignored\n",
    "                             tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                             stop_words=stopwords.words('english')) # stopwords are removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to represent each document in terms of this vector, we use the *fit_transform* function to generate a matrix of documents (the rows) and the vectors with the scores for each words that occurs in each document. The *fit_transform* function creates the model from the vocabulary and applies some transformation to it. You can also use the *fit* function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you might get a warning when you run the following cell. You do NOT have to resolve the warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "airline_counts = airline_vec.fit_transform(airline_tweets_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now created a vector representation *airline_vec* of the complete vocabulary of the full data set. Every position in this vector represents a unique word token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in our data: 653\n"
     ]
    }
   ],
   "source": [
    "#Total number of word features or the length of the total vector\n",
    "print('Total number of words in our data:', len(airline_vec.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '#', '$', '&', \"'\", \"''\", \"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", '(', ')', '-', '--', '.', '..', '...', '....', '1', '10', '100', '15', '1k', '1st', '2', '20', '200', '2015', '24', '25', '2nd', '3', '30', '4', '40', '45', '5', '50', '6', '7', '8', ':', ';', '?', '@', '``', 'aa', 'able']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# First 50 feature names\n",
    "print(list(airline_vec.get_feature_names())[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "443"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'plane' is found in the corpus, mapped to index 1948\n",
    "airline_vec.vocabulary_.get('plane')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect the dimensions of our feature array by getting the spape: the rows (documents) and columns (the word vector length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4755, 653)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# large dimensions! 4,755 documents, 2902 unique terms. \n",
    "airline_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to inspect the dimensions of the data matrix. We see that we have 4,755 text instances with each a vector array of 2,902 positions. Each position represents a word from the training data for which we can set a value that indicates if or how often it occurs in a text instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert the matrix to an array and get the first element and look at the vector values for slots 100 till 200:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(airline_counts.toarray()[0][10:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most values are zero's and just a few have the value 1. This is what we call a *sparse vector*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in the previous Lab, we can also transform the counts into information value scores using the *TfidfTransformer* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert raw frequency counts into TF-IDF values\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "airline_tfidf = tfidf_transformer.fit_transform(airline_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously the shape remains the same but the values are now weighted scores between zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4755, 653)\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.33609914\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.26745264\n",
      " 0.         0.28262499 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.0520561  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.29063094\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.27991468 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.27795445 0.         0.         0.         0.22374514\n",
      " 0.         0.         0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Same dimensions, now with tf-idf values instead of raw frequency counts\n",
    "datarow = 30\n",
    "print(airline_tfidf.shape)\n",
    "# we print values 100:250 for datarow \n",
    "print(airline_tfidf.toarray()[datarow][10:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing a Naive Bayes classifier for airline tweets\n",
    "\n",
    "We can now use the above data representation as training data to build a classifier. the Sklearn package already associated each row (a document) in our data represenation with a label by taking the name of the data subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label for data at row: 0\n",
      "Name for the label as number: negative\n"
     ]
    }
   ],
   "source": [
    "print(\"Label for data at row:\", airline_tweets_train.target[datarow])\n",
    "print(\"Name for the label as number:\", airline_tweets_train.target_names[airline_tweets_train.target[datarow]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a simple Naive Bayes classifier to train a model. A Naive Bayes (NB) classifier is a so-called generative classifier that learns the probablity of a word given a category. By assessing the words a text it can aggregate how likely a text is for a category. You will learn more about this type of classifier in the machine learning code. For now it is enough to know that NB work reasonably for smaller data sets and with little training.  \n",
    "\n",
    "Because we have multiple labels (negative, positive, neutral) and therefore are dealing with 3 classes, we need a *multinomial classifier*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now ready to build a classifier. \n",
    "# We will use Multinominal Naive Bayes as our model\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy for machine learning package to read the above vector representations and associated these with any type of label. However, we also want to test the data. For that purpose, we need to exclude part of the data from a training set.\n",
    "\n",
    "To train the classifier, we will first split the data into train and test. Sklearn has a built in function for this that we need to import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "# from sklearn.cross_validation import train_test_split  # deprecated in 0.18\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose 80% training and 20% test. The function returns 4 different data structures that we need to catch: the training texts, the test texts, the training labels and the test labels. In Python, we can assign these to four different variables in one command call. The *train_test_split* function requires as input the model, the data and the train-test proportional split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train, docs_test, human_labels_train, human_labels_test = train_test_split(\n",
    "    airline_tfidf, # the tf-idf model\n",
    "    airline_tweets_train.target, # the tweets and their labels\n",
    "    test_size = 0.20 # we use 80% for training and 20% for development\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now find our training texts in *docs_train* and our test texts in *docs_test*. The corresponding sequence of labels for training are in *human_labels_train*, and those for testing are in *human_labels_test*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training texts: 3804\n",
      "Training labels: 3804\n",
      "Test texts: 951\n",
      "Test labels: 951\n"
     ]
    }
   ],
   "source": [
    "print(\"Training texts:\", len(docs_train.toarray()))\n",
    "print(\"Training labels:\", len(human_labels_train))\n",
    "print(\"Test texts:\", len(docs_test.toarray()))\n",
    "print(\"Test labels:\", len(human_labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have an equal numer of texts and labels for train and test. The values should be in the right order.\n",
    "\n",
    "One training instance looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.68023261, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.13505632, 0.68046026, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.23667958, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_train[55].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it's label is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_labels_train[55]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which we know is then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_tweets_train.target_names[human_labels_train[55]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *fit* function of sklearn takes as input the training data and the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Multimoda Naive Bayes classifier\n",
    "airline_tweets_clf = MultinomialNB().fit(docs_train, human_labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the classifier, we can apply it to test data that is represented in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results, find macro recall\n",
    "system_labels_pred = airline_tweets_clf.predict(docs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now obtain each test tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet number: 3823\n",
      "The test tweet: b'\"Bruh...real tweet from @JetBlue \"\"Our fleet\\'s on fleek. http://t.co/dqny4aKTg9\"\"\"'\n",
      "one test tweet representation: [[0.20108888036631759, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1551587774718064, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16845021625429452, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5526512012826821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2992999815345262, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.557454253177443, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44872538038200643, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "gold label: 2\n",
      "classifier predicted: 2\n"
     ]
    }
   ],
   "source": [
    "test_item = 20\n",
    "#### we need to get the text from the original text data before we did the split.\n",
    "#### Remember that the nTH item in an array has an index of n-1 because we start the index with zero.\n",
    "#### Likewise, we take the length of the training data minus 1, to find the start of the test data.\n",
    "data_index = len(docs_train.toarray())-1+test_item\n",
    "print('Tweet number:', data_index)\n",
    "print('The test tweet:', airline_tweets_train.data[data_index])\n",
    "print('one test tweet representation:', docs_test[test_item-1].toarray().tolist())\n",
    "print('gold label:', human_labels_test[test_item-1])\n",
    "print('classifier predicted:', system_labels_pred[test_item-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring the quality of the system output against the human labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn provides functions to obtain the recall, precision and f-measure for the test set results. We can simply pass the human labels and the system labels as input and set a parameter how to aggregate the overall score. Separate functions are provide for the recall, precision and f-measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8233438485804416"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.recall_score(y_true=human_labels_test,\n",
    "                             y_pred=system_labels_pred,\n",
    "                             average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification_report function gives the complete result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.810     0.890     0.848       364\n",
      "           1      0.865     0.693     0.770       313\n",
      "           2      0.807     0.883     0.843       274\n",
      "\n",
      "    accuracy                          0.823       951\n",
      "   macro avg      0.827     0.822     0.820       951\n",
      "weighted avg      0.827     0.823     0.821       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(human_labels_test,system_labels_pred,digits = 3)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever you generate a report such as this, don't just stare at the overall performance such as *accuracy* but consider also the results per category and what the relation is between recall and precision. Ideally, recall and precision are balanced but if they deviate it is worth thinking about the causes. Some good causes are:\n",
    "\n",
    "<ol>\n",
    "<li>Not enough training data to capture all variation in the test data\n",
    "<li>Biased training data that makes one category stronger than the other\n",
    "<li>Too much ambiguity so that words as features have different interpretions in different contexts\n",
    "<li>Over-fitting to the training data so that it learns features that do not generalise to the test data\n",
    "<li>Many others....\n",
    "</ol>\n",
    "\n",
    "During this master programme, you will work with many different data sets and analyse the performance of classifiers. Analysing different data, experimenting with text representations, settings and machine learning algorithm is the best way to learn about possible causes and how to mitigate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of a Naive Base classifier, we can easily inspect the least and most important features per category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in negative documents\n",
      "0 156.93427869254464 united\n",
      "0 112.1616409687085 .\n",
      "0 96.91903066761333 @\n",
      "0 93.27171751986671 ``\n",
      "0 57.0315819812597 flight\n",
      "0 50.203424483318386 ?\n",
      "0 44.273490871415554 !\n",
      "0 43.86464647317689 #\n",
      "0 38.72394459013907 n't\n",
      "0 27.717300421533164 ''\n",
      "-----------------------------------------\n",
      "Important words in neutral documents\n",
      "1 105.24952188894775 @\n",
      "1 84.30196140256247 ?\n",
      "1 62.434258876875454 jetblue\n",
      "1 58.56264189918974 southwestair\n",
      "1 57.17709947137887 .\n",
      "1 57.03907744236674 ``\n",
      "1 54.57084502075883 americanair\n",
      "1 52.52222634150541 :\n",
      "1 41.9752297371489 usairways\n",
      "1 37.63956254791795 flight\n",
      "-----------------------------------------\n",
      "Important words in positive documents\n",
      "2 171.6494499723948 !\n",
      "2 106.11764629892475 @\n",
      "2 88.15409338193925 .\n",
      "2 83.85020396837129 thanks\n",
      "2 79.31943694585807 thank\n",
      "2 68.36570020375369 jetblue\n",
      "2 67.83584928111172 southwestair\n",
      "2 65.79361585443769 ``\n",
      "2 50.10652350217669 #\n",
      "2 49.18628819696222 americanair\n"
     ]
    }
   ],
   "source": [
    "def important_features_per_class(vectorizer,classifier,n=10): #n is the number of top features\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names =vectorizer.get_feature_names()\n",
    "    topn_class1 = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]\n",
    "    topn_class2 = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
    "    topn_class3 = sorted(zip(classifier.feature_count_[2], feature_names),reverse=True)[:n]\n",
    "    print(\"Important words in negative documents\")\n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels[0], coef, feat)\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in neutral documents\")\n",
    "    for coef, feat in topn_class2:\n",
    "        print(class_labels[1], coef, feat) \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in positive documents\")\n",
    "    for coef, feat in topn_class3:\n",
    "        print(class_labels[2], coef, feat) \n",
    "\n",
    "# example of how to call from notebook:\n",
    "important_features_per_class(airline_vec, airline_tweets_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think about these most important words? According to the algorithm these words are most strongly associated with the categories. Is this intuitive to you?\n",
    "\n",
    "By inspecting the performance and the feature analysis, you will start gaining insight in the data and the system. Hopefully this inspires you how to improve the system performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the airline tweets classifier on your own data\n",
    "Now we can apply our classifier to new data. Let's type a few potential tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# very short and fake movie reviews\n",
    "my_reviews = ['This movie was excellent', \n",
    "               'Absolute joy ride', \n",
    "               'Steven Seagal was terrible', \n",
    "               'Steven Seagal shined through.', \n",
    "               'This was certainly a movie', \n",
    "               'Two thumbs up', \n",
    "               'I fell asleep halfway through', \n",
    "               \"We can't wait for the sequel!!\", \n",
    "               'I cannot recommend this highly enough', \n",
    "               'instant classic.', \n",
    "               'Steven Seagal was amazing.']\n",
    "len(my_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to apply our model to these text, we need to represent the text using the same vector dimensions (columns!) as we used for training the model.\n",
    "\n",
    "The sklearn transformer function does this work for you using the **transform** function. This function takes the vectors for representing the training data. Note that these vectors have a position for every word in the training data. So we cannot represent words that do not occur outside the training data, so-called Out-of-Vocabulary words. Technically, we do not need to worry because the **transform** function will take care of this but you can imagine that texts that are very different from the training data will be represented poorly by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 2902)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We re-use airline_vec to transform it in the same way as the training data\n",
    "my_review_counts = airline_vec.transform(my_reviews)\n",
    "my_review_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the shape of our matrix represents 11 rows using the vectors with 2,907 dimensions that we created from the training data before. So words in our movie reviews that are NOT in the training data, are not represented as there are no slots in the vectors from the training data.\n",
    "\n",
    "We can see this clearly, if we would use the **fit_transform** function instead of **transform** for our tweets, which will create a whole new model from our 11 tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 4)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_my_review_counts = airline_vec.fit_transform(my_reviews)\n",
    "bad_my_review_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we only have 4 dimensions left because the CountVectorizer created complete new vector representations using our settings: min_df=2,tokenizer=nltk.word_tokenize, stop_words=stopwords.words('english'). The vector only represents the words from my_reviews and this representation is incompatible with our trained model. So we cannot use this representation!!\n",
    "\n",
    "This is a mistake you are likely to make. Remember:\n",
    "\n",
    "1. build a vectore representation from the features from the training data: use **fit** or **fit_transform** with your defined CountVectorizer instance\n",
    "2. train a classifier from these representation\n",
    "3. always, always use the **transform** function from your defined CountVectorizer instance (which has been derived using the training data) to represent any other texts thatr you want to classify\n",
    "\n",
    "It is good practice to always check the **shape** function if the texts are represented using the same number of dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that our own review texts are now propery represented in my_review_counts*my_review_counts*, we can proceed with calculating **TF\\*IDF** values and use our trained classifier to make predictions on these representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we compute tf idf values\n",
    "my_reviews_tfidf = tfidf_transformer.transform(my_review_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 2902)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_reviews_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These dimensions are correct so let's get the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have classifier make a prediction\n",
    "pred_on_my_reviews = airline_tweets_clf.predict(my_reviews_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie was excellent => positive\n",
      "Absolute joy ride => positive\n",
      "Steven Seagal was terrible => negative\n",
      "Steven Seagal shined through. => negative\n",
      "This was certainly a movie => negative\n",
      "Two thumbs up => negative\n",
      "I fell asleep halfway through => neutral\n",
      "We can't wait for the sequel!! => negative\n",
      "I cannot recommend this highly enough => negative\n",
      "instant classic. => negative\n",
      "Steven Seagal was amazing. => positive\n"
     ]
    }
   ],
   "source": [
    "# print out results ()\n",
    "for review, predicted_label in zip(my_reviews, pred_on_my_reviews):\n",
    "    \n",
    "    print('%s => %s' % (review, \n",
    "                        airline_tweets_train.target_names[predicted_label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a minute to see if these reviews are correctly classified. What would be needed to evaluate the performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training another classifier with movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we loaded the NLTK movie review data set using the sklearn function *load_files* in the same way as we have done for the airlinetweets. This means we can build another classifier from this data as well and apply it to the same set of *my_reviews* and compare the two systems.\n",
    "\n",
    "We proceed in three simple steps:\n",
    "\n",
    "<ol>\n",
    "    <li>We create a CountVectorizer to vectorize the training texts based on the total vocabulary using the *fit_transform* function\n",
    "    <li>We transfer the values using the tfidf_transformer into information values\n",
    "    <li>We create a *MultinomialNB* classifier from the vectorised documents and their labels\n",
    "</ol>\n",
    "\n",
    "Once we trained the classifier, we can apply it to the above examples by transforming the my_review texts to this reptentation to make them compatible to our new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_vec = CountVectorizer(min_df=2, # If a token appears fewer times than this, across all documents, it will be ignored\n",
    "                             tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                             stop_words=stopwords.words('english')) # stopwords are removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this instance of CountVectorizer, we can now derive a new (and very different) vector representation from the movie_review data we loaded before. We use here the **fit_transform** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "movie_counts = movie_vec.fit_transform(movie_reviews_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use **shape** to see that our matrix has 2000 rows for the data, which are now represented through 25,138 dimensions. So this is a very different vector representation than derived from the airline tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 25138)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the count values to **TF\\*IDF** values as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_tfidf = tfidf_transformer.fit_transform(movie_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sklearn load_files functions not only return the data for training but also the classification labels that go with it based in the folder names that contain the text files. The *target* attribute contains the lists of values as integer indexes corresponding to each document representation in the data and *target_names* contain the meaning of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n",
      "[0 1 1 0 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(movie_reviews_train.target_names)\n",
    "# print the labels for the first ten documents\n",
    "print(movie_reviews_train.target[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_review_clf = MultinomialNB().fit(movie_tfidf, movie_reviews_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying the movier review classifier to your own data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply our new classifier to the same my_reviews data, we first need to represent these texts according to the vectors of the movie_review data. So, we call again the **transform* function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 25138)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We re-use airline_vec to transform it in the same way as the training data\n",
    "my_review_movie_review_counts = movie_vec.transform(my_reviews)\n",
    "my_review_movie_review_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_new_tfidf = tfidf_transformer.transform(my_review_movie_review_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_review_clf_pred = movie_review_clf.predict(reviews_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie was excellent => pos\n",
      "Absolute joy ride => pos\n",
      "Steven Seagal was terrible => neg\n",
      "Steven Seagal shined through. => neg\n",
      "This was certainly a movie => neg\n",
      "Two thumbs up => neg\n",
      "I fell asleep halfway through => neg\n",
      "We can't wait for the sequel!! => neg\n",
      "I cannot recommend this highly enough => pos\n",
      "instant classic. => pos\n",
      "Steven Seagal was amazing. => neg\n"
     ]
    }
   ],
   "source": [
    "# print out results ()\n",
    "for review, predicted_label in zip(my_reviews, movie_review_clf_pred):\n",
    "    \n",
    "    print('%s => %s' % (review, \n",
    "                        movie_reviews_train.target_names[predicted_label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our moview review classifier only uses two categories instead of the airlinetweets classifier. Is it doing any better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
