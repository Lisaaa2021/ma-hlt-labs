{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3.1 Machine learning basics\n",
    "\n",
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL\n",
    "\n",
    "RMA/Text Mining MA, Introduction to HLT\n",
    "\n",
    "This notebook explains the simple basics of machine learning. At the end of this notebook, you learned:\n",
    "\n",
    "- the basic principles of machine learning for text classification\n",
    "- how features are represented as vectors\n",
    "- how to train a classifier from vector representations\n",
    "- how to train and apply a classifier to text represented by its words\n",
    "- what a bag-of-words representation is\n",
    "- what the information value (TF*IDF) of a word is\n",
    "\n",
    "**Background reading:**\n",
    "\n",
    "NLTK Book\n",
    "Chapter 6, section 1 and 3: https://www.nltk.org/book/ch06.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Machine Learning schema\n",
    "\n",
    "The overall process of machine learning is shown in the next image that is taken from Chapter 6 of the NLTK book. In general, machine learning consists of a training phase in which an algorithm associates data features with certain labels (e.g. sentiment, part-of-speech). The training results in a classifier model that can be applied to unseen data. The classifier compares the features of the unseen data with the previously seen data and makes a prediction of the label on the basis of some similarity calculation.\n",
    "\n",
    "![title](images/ml-schema.png)\n",
    "\n",
    "\n",
    "Crucial in this process is 1) the features that represent the data and 2) the algorithm that is used. In this course, we are not going to discuss the various machine learning algorithms in depth but we focus on the text features and how they are represented as so-called feature vectors. In the case of a text, we need to define what the features are that characterize the text. These features are transformed into a feature vector representation that the algorithm and model can handle. In order to compare unseen text with the training texts, it is crucial that features are extracted and represented in the same way across training and applying (among which testing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparations**\n",
    "\n",
    "We are going to use the Scikit-learn package to transform the feature values into a vector representation:\n",
    "\n",
    "https://scikit-learn.org/stable/install.html\n",
    "\n",
    "Scikit-learn is a package that contains a lot of machine learning algorithms and functions for dealing with features and carrying out evaluation and error analysis. To install it run one of the following commands from the command line:\n",
    "\n",
    "- pip install -U scikit-learn\n",
    "\n",
    "or\n",
    " \n",
    "- conda install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also using a package called \"numpy\": https://numpy.org. *Numpy* is a package for representing numerical and vector represenrations in Python.\n",
    "\n",
    "Install \"numpy\" from the command line following the instructions on the website. After installing, you can import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Vector representations\n",
    "\n",
    "\n",
    "Before we turn to a text example, we are going to use a very simple data set. We show how to train and evaluate an SVM (Support-Vector-Machine) using a made-up example of multi-class classification for a non-linguistic dataset. The goal is to predict someone's weight category (say: skinny, fit, average, overweight) based on their properties.\n",
    "\n",
    "We use three features:\n",
    "\n",
    "* **age in years**\n",
    "* **height in cms**\n",
    "* **number of ice cream cones eaten per year**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature representation (for 5 people) is an array of arrays (or a matrix). \n",
    "\n",
    "This is a single array: [1,2,3]\n",
    "\n",
    "If we put several of these in a list [[1,2,3,4],[5,6,7,8]] we have an array of arrays. Each array in the list represents the data for a single person.\n",
    "\n",
    "Each row (or person) is represented by an array of numbers in which the first is the age, the second the height in cms and the third the number of cones per year: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[30, 180, 1000], \n",
    "     [80, 180, 100],\n",
    "     [50, 180, 100],\n",
    "     [40, 160, 500],\n",
    "     [15, 160, 400]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first person is thus 30 years old, 180 cms tall and eats 1000 cones per year. The next command prints the data for the first instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First instance in the data set X = [30, 180, 1000]\n"
     ]
    }
   ],
   "source": [
    "print('First instance in the data set X =', X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An array of numbers in which each position holds a value for a specific feature is what we call a feature vector. For all our data in the data set we must have a feature vector of the same length. If there is no value, it will be zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the data that is now assigned to the variable 'X', we also need to have the prediction that goes with the instances. For this we use another array with the values that we assign to the variable 'Y'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = [\"overweight\", \n",
    "     \"skinny\",\n",
    "     \"fit\",\n",
    "     \"average\",\n",
    "     \"average\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to have as many values as we have instances in our data set, as the software pairs the elements in X with the elements in Y. Obviously, the values should also be in the correct order to correspond with the instances!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the data set = 5\n",
      "The length of the predictions = 5\n",
      "The first prediction = overweight\n"
     ]
    }
   ],
   "source": [
    "print('The length of the data set =', len(X))\n",
    "print('The length of the predictions =', len(Y))\n",
    "print('The first prediction =', Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice function to pair lists in Python is the \"zip\" function which creates a list of tuples from two lists. We can use this to pair the instances with their labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30, 180, 1000] overweight\n",
      "[80, 180, 100] skinny\n",
      "[50, 180, 100] fit\n",
      "[40, 160, 500] average\n",
      "[15, 160, 400] average\n"
     ]
    }
   ],
   "source": [
    "for instance, label in zip(X, Y):\n",
    "    print(instance, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Using Skikit learn to build a classifier\n",
    "\n",
    "Since we have the data and the prediction, we can train a model. We are going to use the **svm** module from **sklearn**, from which we will select the **LinearSVC** (Linear Support Vector Classification: https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html) class. Support Vector Machines or SVMs are powerful supervised machine learning approaches that find the optimal division (a so-called hyperplane in a multidimensional data space) between positive and negative examples of a class. For now it is not important to know the details about this algorithm. You will learn about that in the machine learning course. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import *svm* from sklearn and next we instantiate a model with the variable name 'lin_classifier' (any name will do and you can instantiate as any variables as you want until your run out of memory). We will use this instantiation for training and classifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "lin_classifier = svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model by feeding it with the data set 'X' and the predictions 'Y'. Feeding we do with the 'fit' function. The 'fit' function creates the model and adds the data to it. The model is defined by the number of properties in the data but also by the order of the properties. So the current data example has 3 properties and the first position has the values for the age and not something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_classifier.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the fit function gives a response that shows the (default) parameter settings of this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you train the model through the 'fit' command, you might get a warning stating that:\n",
    "```\n",
    "ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
    "```\n",
    "This is to be expected given that we only train using five instances.\n",
    "\n",
    "The default setting of LinearSVC is to iterate maximally 1,000 times over the data to get convergence. See the documentation:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "\n",
    "What we can try is to increase this by setting the parameter for *max_iter* to 10,000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(max_iter=10000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_classifier = svm.LinearSVC(max_iter=10000)\n",
    "lin_classifier.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the warning did not disappear. The data set is really too small for convergence, even after 10,000 iterations. We leave it for now. You will learn more about SVM classiciers in the Machine Learning course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Using Sklearn to classify unseen data\n",
    "\n",
    "Let's now apply the model to a new instance 'Z'. What does our trained SVM instance think about the weight category of an instance who is 18 years old, 171cm tall, and who eats 400 ice cream cones per year? We can use the *predict* function of our classifier instance to tell is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['average']\n"
     ]
    }
   ],
   "source": [
    "Z=[[18, 171, 400]]\n",
    "predicted_label = lin_classifier.predict(Z)\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently the SVM instance thinks it is **average**, which is not surprising since **number of ice cream cones eaten per year** and **height** seem to correlate highly with the weight categories.\n",
    "\n",
    "Note that we people reason with some (weak) causal explanatory model but our Machine Learning model just uses data patterns and association. It does not know why the answer *average* makes sense or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Representing a text as a Bag-Of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now let's move from people to text. When we process language, we typically want to look at instances of text and to represent each text by the linguistic features. So in our data structure, each row will be a piece of text (as rows were people in the previous example) and the array will have values for different properties of the text. Instead of predicting the *weight*, we now want to predict interpretations of the text, such as its sentiment or the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A critical component of almost any machine learning approach is **feature representation**. \n",
    "This is not so strange since we need to somehow convert a complex text, e.g., words, sentences, a tweet, or a document, into something numerical that can be interpreted by a computer, and is also useful for the type of learning we want to do.\n",
    "\n",
    "A text consists of a sequence of words on which we impose syntax and semantics. A machine needs to learn to associate the structural properties of the text to some interpretation.\n",
    "We can use various language properties to do this, both structural and external:\n",
    "\n",
    "- the words (regardless of the order or in order)and their frequency in a text\n",
    "- the part-of-speech of words\n",
    "- grammatical relations such as dependencies\n",
    "- combinations of words: sequences of three words, four words, etc. (so-called word n-grams), phrases, sentences\n",
    "- the characters that make up the words (so-called character n-grams)\n",
    "- the meaning of words or combinations of words in a lexicon\n",
    "- word length, sentence length, word position in a text or a sentence\n",
    "- discourse structure: title, header, caption, body, conclusion sections\n",
    "- etc....\n",
    "\n",
    "Which features work often depends on the kind of task and is often determined experimentally.\n",
    "\n",
    "Some of the above structural properties, we can get for free if we split a text into tokens. Other properties are not explicit, such as the part-of-speech of words, phrases, syntax and their meaning.\n",
    "\n",
    "For now, we are only considering the words of a text as features. In fact, we are going to ignore the order of the words and consider a text as a *Bag-Of-Words*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you want to learn more: (information from these blogs was used in this notebook)**\n",
    "* [bag of words introduction](http://www.insightsbot.com/blog/R8fu5/bag-of-words-algorithm-in-python-introduction)\n",
    "* [TF-IDF introduction](https://medium.freecodecamp.org/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3)\n",
    "* [another TF-IDF introduction](https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/)\n",
    "\n",
    "In the machine learning course, we explain how other features can be combined with a word representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Bag of words\n",
    "\n",
    "We are going to create a vector representation of a text in which the words are the features that characterize the content of the text. To keep things simple, we ignore the order of the words but we do want to know how often a word occurs in a document so that we can give it a weight.\n",
    "\n",
    "In our vector representation we want each word to occupy a unique position in the array just as the age [0], length [1] and number of cones [2] in our *weight* prediction example. That means that our vector needs to be as long as the number of words that we find in the text. \n",
    "\n",
    "The first thing we therefore need to do is to create a word-to-document index:\n",
    "\n",
    "* we extract all the unique words from a collections of textual units, e.g., documents or tweets\n",
    "* we compute the frequency of each word in each document\n",
    "\n",
    "Knowing the unique vocabulary of the documents, we can create a vector array with the length of the vocabulary and the order of the words in our vocabulary corresponds with the order in the array. \n",
    "\n",
    "Next, we can represent each document by the vector array by adding a row for a document (an instance of a text) where we score each position with the frequency of this word in the text. Instead of just counting each word, we can also determine the information value of the word for the document as the *TF.IDF* value.\n",
    "\n",
    "Let's look at an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do all the above, we need two modules from sklearn that do all the work:\n",
    "\n",
    "* CountVectorizer: turns a text data set into a vector representation consisting of a vector array and a vocabulary that relates each data point to the corresponding vector array position\n",
    "* TfidfTransformer: calculates the *TF.IDF* values from the basic statistics\n",
    "\n",
    "We also need the NLTK package from the previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this for the following three sentences that we list in an array (note that sentences can also be complete documents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = ['A rose is a rose',\n",
    "         'A rose stinks',\n",
    "         \"A book is nice\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have three instances of text with words occurring across the texts and different frequencies in the text. We will use the **CountVectorizer** function to create a bag of words representation from the above arrays. It requires two parameters to be set in advance when we create an instance of the CountVectorizer: 1) the minimal number of documents in which the term should occur (rare words are ignored) and 2) what tokenizer should be used. Since our data set is small, we set the minimal number of documents to \"1\". As a tokenizer to split the text, we use NLTK.\n",
    "\n",
    "We first create the instance *our_vectorizer* and feed it with our sentences to derive the data arrays for the instances with the function *fit_transform*. This will give us two things:\n",
    "\n",
    "* a data structure that represents the instances through their vectors\n",
    "* the vocabulary that maps to the columns of the data structure\n",
    "\n",
    "The result of calling this function is assigned to the variable *sents_vector_data*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can adapt min_df to restrict the representation to more frequent words e.g. 2, 3, etc..\n",
    "\n",
    "our_vectorizer = CountVectorizer(min_df=1, # in how many documents the term minimally occurs\n",
    "                             tokenizer=nltk.word_tokenize) # we use the nltk tokenizer to split the text into tokens\n",
    "sents_vector_data = our_vectorizer.fit_transform(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's us now inspect the sents_vector_data object created by *our_vectorizer*. The data representation of the text is assigned to the variable *sents_vector_data*. The vocabulary is stored in the *our_vectorizer*.\n",
    "\n",
    "We first look at *sents_vector_data*. It is a special sklearn Object (csr_matrix) for which there are many functions and attibutes defined. We are going to look at the *shape* which holds the data. Printing the so-called \"shape\" of sents_counts shows us that we have 3 documents and 6 unique words spread over these documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(3, 6)\n",
      "The vector representation of the sentences looks as follows:\n",
      "[[2 0 1 0 2 0]\n",
      " [1 0 0 0 1 1]\n",
      " [1 1 1 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(type(sents_vector_data))\n",
    "print(sents_vector_data.shape)\n",
    "# sents_counts has a dimension of 3 (document count) by 6 (# of unique words)\n",
    "\n",
    "print('The vector representation of the sentences looks as follows:')\n",
    "print (sents_vector_data.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!! That looks very similar to the numerical data that we used to train our SVM for predicting the weight of people with certain features. Now the columns stand for words and the rows are the sentences or pieces of text.\n",
    "\n",
    "Important to note is that the rows are longer than any sentence we gave it because they represent the complete vocabulary of all the sentences. That's why the texts have zero values in their representation for words that do not occur in it but occur in other texts.\n",
    "\n",
    "Let's check the vocabulary now, which stored in *our_vectorizer*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary of all the sentences  consists of the following words: ['a', 'rose', 'is', 'stinks', 'book', 'nice']\n",
      "These words are mapped to the data columns as feature names: ['a', 'book', 'is', 'nice', 'rose', 'stinks']\n"
     ]
    }
   ],
   "source": [
    "# this vector is small enough to view in full! \n",
    "print('The vocabulary of all the sentences  consists of the following words:', list(our_vectorizer.vocabulary_.keys()))\n",
    "print('These words are mapped to the data columns as feature names:', our_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the feature name, we can now recover the three texts from the previous data array.\n",
    "\n",
    "The first array has 6 positions representing the complete vocabulary. The first position represents the first word \"a\" and it has value '2', which means it occurs twice in the sentence. The fourth slot is for \"is\" which occurs once and the fifth slot is for \"rose\" which occurs twice. The other slots are zero because these words do not occur in the first sentence.\n",
    "\n",
    "Try to figure out if you understand the representation of the other two sentences!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Training a classifier with word vectors\n",
    "Now we have seen how we can turn a text into a vector representation. We can associate these text representations with labels as we have seen above for predicting somebody's weight. We now use different labels but note that for the algorithm the labels are meaningless. They could be numbers or any label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not so difficult to see how we can train an SVM instance with these data. All we need is to pair a set of labels to the data instances. Let's use sentiment values: neutral, negative and positive. You can also use other labels such as A, B, C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_labels=[\"neutral\", \"negative\", \"positive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 1 0 2 0] neutral\n",
      "[1 0 0 0 1 1] negative\n",
      "[1 1 1 1 0 0] positive\n"
     ]
    }
   ],
   "source": [
    "for instance, label in zip(sents_vector_data.toarray(),sentiment_labels):\n",
    "    print(instance, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have nicely paired sentence representations and sentiment values. Let's train and test an SVM. To train the model, we use the *fit* function of the svm again as before. We feed it with the sents_vector_data generated by *our_vectorizer* with the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "lin_classifier = svm.LinearSVC()\n",
    "lin_classifier.fit(sents_vector_data,sentiment_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Classifying a new text with our text classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to apply this model to a new text. We need to create a vector representation for this text as well but we can ONLY(!!!) use the words from the training data since the vectors need to have the same semantics as the training data. We cannot represent words that are not in the training data in our model. Furthermore, we need to represent the words that do in the right order. The feature names stored in the vectorizer present the vocabulary in the right order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'book', 'is', 'nice', 'rose', 'stinks']\n"
     ]
    }
   ],
   "source": [
    "new_text=\"a good book is a rose\"\n",
    "print(our_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thus need to create an array with the length of the training vocabulary and add the counts of these words on the basis of the new text. This would look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text_vector=[[2, 1, 1, 0, 1, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also obtain this representation by using the vectorizer we created before, and converting the csr object to an array. We use the function *transform* which transforms data into the representation of a given model.\n",
    "\n",
    "**Note (!!!!)** that we do not use *fit_transform* here. What will happen if you did? Think about it ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "new_text_vector = our_vectorizer.transform([new_text]).toarray()\n",
    "print(new_text_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the word \"good\" is not represented as it does not occur in the training vocabulary. The word \"a\" occurs twice, \"book\" and \"is\" occur once, \"nice\" and \"stinks\" do not occur and \"rose\" also occurs once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral']\n"
     ]
    }
   ],
   "source": [
    "predicted_label = lin_classifier.predict(new_text_vector)\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction is *neutral* which makes sense since none of the distinguishing words \"nice\" and \"stinks\" occur in the text. So let's manipulate the data and turn the value for \"stinks\" to \"1\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text_vector=[[2, 1, 1, 0, 1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative']\n"
     ]
    }
   ],
   "source": [
    "predicted_label = lin_classifier.predict(new_text_vector)\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That did help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 TF-IDF\n",
    "One big problem of the bag of words approach is that it treats all words as equal. Why is that a disadvantage? It means that words that occur in many documents, such as *a*, contribute equally to the decision making of the machine learning approach as other words that are much more informative, e.g., *rose*. \n",
    "TF-IDF addresses this problem by assigning less weight to words that occur in many documents.\n",
    "You read [here](https://medium.freecodecamp.org/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3) a nice introduction to TF-IDF. \n",
    "\n",
    "TF-IDF comes from the information retrieval research. You can image that it matters to measure how discriminative words are across different documents. For other applications such as sentiment classification, it is less clear what the contribution will be. Here we are going to apply it anyway and see what it does to the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how you can do it in Python using sklearn. We create an instance of the special class TfidTransformer to feed it the bag-of-words presentation that we created before for training with *our_vecitorizer*. This instance also has a *fit_transform* function that now produces a representation with the TF-IDF values for the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "sents_tfidf = tfidf_transformer.fit_transform(sents_vector_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inspect the result, we need to convert it to an *array*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'book', 'is', 'nice', 'rose', 'stinks']\n",
      "[[0.57048339 0.         0.36730061 0.         0.73460123 0.        ]\n",
      " [0.42544054 0.         0.         0.         0.54783215 0.72033345]\n",
      " [0.34520502 0.5844829  0.44451431 0.5844829  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tf_idf_array = sents_tfidf.toarray()\n",
    "print(our_vectorizer.get_feature_names())\n",
    "print(tf_idf_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get rounded values, we can use a *numpy* function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'book', 'is', 'nice', 'rose', 'stinks']\n",
      "[[0.57 0.   0.37 0.   0.73 0.  ]\n",
      " [0.43 0.   0.   0.   0.55 0.72]\n",
      " [0.35 0.58 0.44 0.58 0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(our_vectorizer.get_feature_names())\n",
    "print(numpy.round(tf_idf_array, decimals=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This is a good result! \n",
    "\n",
    "In the bag of words approach, The words **\"a\"** and **\"book\"** both had a frequency of 1 in the third sentence. Now that we've applied the TF-IDF approach, we see that the word *book* has a higher weight (0.58) than the word \"*a*\" in the third text since \"*a*\" occurs in all three sentences and \"*book*\" only in one, which might indicate that it is more informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try again training a model with the new data representation. Since we want to replace the old model, we use the function *fit_transform*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_classifier_weight = svm.LinearSVC()\n",
    "lin_classifier_weight.fit(tf_idf_array,sentiment_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to test the new sentence we must apply the same transformations we did to the training data. This means we have to also represent the new sentence using TF-IDF features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'book', 'is', 'nice', 'rose', 'stinks']\n",
      "[[0.63 0.53 0.4  0.   0.4  0.  ]]\n"
     ]
    }
   ],
   "source": [
    "#redefine new test without manipulation\n",
    "new_text_vector=[[2, 1, 1, 0, 1, 0]]\n",
    "\n",
    "# transform the counts to tf-idf features\n",
    "new_tf_idf_text_vector = tfidf_transformer.transform(new_text_vector)\n",
    "\n",
    "new_tf_idf_array = new_tf_idf_text_vector.toarray()\n",
    "print(our_vectorizer.get_feature_names())\n",
    "print(numpy.round(new_tf_idf_array, decimals=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that although the word \"*book*\" is assigned a high weight (0.53), the TF-IDF approach still gets confused with the test data and assigns a higher weight to the word \"*a*\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use the new trained model with the new representations to get a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral']\n"
     ]
    }
   ],
   "source": [
    "predicted_label = lin_classifier_weight.predict(new_tf_idf_text_vector)\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, the difference in representation did not lead to a different prediction. \n",
    "\n",
    "It is important to see two things in this example:\n",
    "\n",
    "<ol>\n",
    "<li> Regardless of the changes we made in representing the data, the model cannot process words that it has not seen, such as \"*good*\". \n",
    "<li> The model does not know that unseen words can be **semantically** related to words that it has seen. For example, the model does not know that the word \"*good*\" is related to \"*nice*\".\n",
    "</ol>\n",
    "\n",
    "One way to fix the these problems would be by having a larger (training) dataset with more diverse words. However, given the (small) data the model has seen so far, perhaps the prediction 'neutral' is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
